# Forgekeeper (Fresh Start)

Quick CLI entry points and scripts to bring up the Core (llama.cpp by default), ensure the full stack, and chat with reasoning.

## Quick Start

- Copy environment template and adjust as needed:
  - `cp .env.example .env`

- Choose inference core (default is llama.cpp):
  - Edit `.env` and set `FK_CORE_KIND=llama` (default) or `FK_CORE_KIND=vllm`.

- Ensure Core only (idempotent):
  - Windows: `pwsh scripts/ensure_llama_core.ps1`
  - Linux/mac: `bash scripts/ensure_llama_core.sh`
  
- Ensure full stack (profiles + optional MongoDB):
  - Windows: `python -m forgekeeper ensure-stack --build --include-mongo`
  - Linux/mac: `python -m forgekeeper ensure-stack --build --include-mongo`
  - Use `--compose-file PATH` only when targeting a non-default Compose file (defaults to `docker-compose.yml`).

- Chat
  - Python CLI (streaming default): `python -m forgekeeper chat -p "Say 'harmony ok'."`
  - Python CLI non-stream: `python -m forgekeeper chat -p "Hello" --no-stream`
  - Tools demo (safe dir listing): `python -m forgekeeper chat -p "List current folder" --tools dir`
  - PowerShell script (Windows):
    - Streaming: `pwsh forgekeeper/scripts/chat_reasoning.ps1 -Prompt "Hello"`
    - Non-stream: `pwsh forgekeeper/scripts/chat_reasoning.ps1 -Prompt "Hello" -NoStream`

Streaming vs non‑stream
- Streaming prints tokens as they arrive via SSE; you’ll see partial reasoning chunks `[r]` and a final `[final]` line.
- Non‑stream waits for the response to finish and prints only the final text.

System prompt
- UI: open the "Assistant System Prompt" panel to switch between the auto‑generated prompt (includes tool list) and a custom prompt. The custom prompt is saved to localStorage and applied to the first system message in the chat.
- CLI: pass a one‑off override with `--system "..."` when using `python -m forgekeeper chat`.

### No‑GPU mock for smoke tests
- Start a local mock OpenAI server and run the smoke script:
  - `node scripts/mock_openai_server.mjs` (serves `/v1/chat/completions`, `/health[z]`)
  - In another shell: `FK_CORE_API_BASE=http://localhost:8001 python scripts/test_harmony_basic.py`

### Make targets
- `make dev-ui` - run Vite dev server
- `make ui-build` - typecheck + build UI
- `make lint` - ESLint on `src/`
- `make typecheck` - `tsc --noEmit`
- `make test-ui` - vitest (server orchestrator)
- `make test-py` - install + pytest
- `make task-sanity` - lint task cards for required fields
- `make pr-check TASK=T#` - locally enforce Allowed Touches for staged changes

## Frontend (Web UI)

- Dev server (Vite + React):
  - Install deps: `npm --prefix frontend install`
  - Start: `npm --prefix frontend run dev`
  - Opens on `http://localhost:5173` and proxies `/v1`, `/health`, `/healthz` to the Core server.
  - Note: `/api/chat` is not available in Vite dev mode; use the server mode below.

- Tool-enabled chat orchestration (server-side):
  - Endpoint: `POST /api/chat` (non-streaming) via `frontend/server.mjs`.
  - Client helper: `frontend/src/lib/chatClient.ts::chatViaServer`.
  - Tools live under: `frontend/tools/*.mjs` with aggregator `tools/index.mjs` (compat wrapper at `server.tools.mjs`).
  - Orchestrator loop: `frontend/server.orchestrator.mjs` handles `tool_calls`.
  - UI wiring: `Chat.tsx` routes blocking sends via `/api/chat` and streaming sends via `/api/chat/stream` automatically (no separate tools button).
  - Discovery: `GET /api/tools` returns `{ enabled, count, names, defs }`; `/config.json` includes a `tools` summary. The UI disables tools when none available and shows the list in the footer.
  - System prompt: `Chat.tsx::buildSystemPrompt` uses the `/api/tools` metadata (falling back to the names list) to describe each tool. Update that helper if tool usage instructions change so the UI and allowlist stay in sync.

### Built-in Tools (Server)
- `get_time`: returns current UTC ISO timestamp.
- `echo`: echoes provided text.
- `read_file`: reads a text file under the sandbox root (size-limited).
- `read_dir`: lists directory entries under a sandbox root.
- `write_file`: writes a file under the sandbox root (size-limited).
- `run_powershell` (gated): runs a PowerShell command if enabled.

Environment controls (server.mjs process):
- `TOOLS_FS_ROOT` — sandbox root for `read_dir`/`write_file`.
  - Default: current working dir of the server process.
  - Docker: docker-compose sets `TOOLS_FS_ROOT=/workspace/sandbox` so files persist in the bind-mounted repo directory (`./ -> /workspace`).
  - Override in `.env` if you want another persisted path (e.g., `/workspace/data`).
- `TOOLS_MAX_WRITE_BYTES` — max bytes for `write_file` (default: 65536).
- `TOOLS_MAX_READ_BYTES` — max bytes for `read_file` (default: 65536).
- `FRONTEND_ENABLE_POWERSHELL=1` — enable `run_powershell` tool (disabled by default).
- `PWSH_PATH` — override pwsh executable (defaults: `pwsh` or `powershell.exe` on Windows).

### Tool Management & Monitoring

**Status**: ✅ Fully implemented (codex.plan Phases 1-2)

**Overview**: Comprehensive tool lifecycle management with approval system, error tracking, regression monitoring, and resource usage.

**Core Features**:
- ✅ **AI-Generated Tool Approval** (T203) - Propose and approve new tools dynamically
- ✅ **Error Statistics Tracking** (T205) - Per-tool error rates, types, and patterns
- ✅ **Regression Monitoring** (T211) - Detect performance degradation over time
- ✅ **Resource Usage Tracking** (T212) - CPU, memory, disk usage per tool

**Key Endpoints** (15+ at `/api/tools/*`):
- Discovery: `GET /api/tools`, `GET /api/tools/config`
- Approval System: `POST /api/tools/propose`, `GET /api/tools/pending`, `POST /api/tools/approve/:tool_name`
- Error Tracking: `GET /api/tools/errors`, `GET /api/tools/errors/:tool_name`, `POST /api/tools/errors/:tool_name/clear`
- Regression: `GET /api/tools/regression`, `GET /api/tools/regression/:tool_name`, `POST /api/tools/regression/:tool_name/clear`
- Resources: `GET /api/tools/resources`, `GET /api/tools/resources/:tool_name`
- Dynamic Loading: `POST /api/tools/reload`, `POST /api/tools/write`

**Environment Variables**:
- `TOOLS_APPROVAL_REQUIRED=1` - Require approval for new tools (default: `0`)
- `TOOLS_APPROVAL_EXPIRES_MS=86400000` - Proposal expiry time (default: 24h)
- `TOOLS_MAX_OUTPUT_BYTES=10240` - Max tool output size (default: 10KB)
- `TOOLS_MAX_OUTPUT_LINES=256` - Max output lines (default: 256)
- `TOOLS_TIMEOUT_MS=30000` - Tool execution timeout (default: 30s)

**Documentation**:
- API Reference: `docs/api/tools_api.md`
- Security: Sandbox isolation, size limits, timeout enforcement
- Auditing: All tool calls logged to ContextLog with full telemetry

### Debugging tools
- In the UI, enable "Tools diagnostics" to see recent tool calls (name + args) per step of the server-side orchestration.
- Diagnostics Drawer: toggle in Chat to view recent ContextLog events for the current `conv_id` (uses `/api/ctx/tail`).
- New: `GET /api/ctx/tail?n=50&conv_id=...` returns recent ContextLog events (JSONL-backed).

### Server Policies & Limits
- Tool allowlist: set `TOOL_ALLOW` to a comma‑separated list of tool names to permit (e.g., `get_time,echo,read_file`). If unset, all registered tools are allowed.
- Rate limiting: set `API_RATE_PER_MIN` to an integer to limit requests per minute per IP for `/api/chat` and `/api/chat/stream` (0 disables limiting).
- Metrics: GET `/metrics` returns counters `{ totalRequests, streamRequests, totalToolCalls, rateLimited }`.
- Auditing: tool executions append JSON lines to `.forgekeeper/tools_audit.jsonl` (fields: `ts`, `name`, `args`, `iter`, `ip`).
- ContextLog: structured JSONL under `.forgekeeper/context_log/` with correlation fields (`conv_id`, `trace_id`, `iter`). See `docs/observability.md` and `docs/contextlog/adr-0001-contextlog.md`.

### Streaming Final Turn
- Non‑streaming tools loop: `POST /api/chat` runs tool orchestration and returns `{ assistant, messages, debug }`.
- Streaming final turn: `POST /api/chat/stream` runs the tool loop server‑side, then streams the final assistant turn from the upstream OpenAI‑compatible server via SSE (`text/event-stream`).
- The Vite dev client can still stream directly from `/v1/chat/completions`; use the “Send (tools)” button to route via `/api/chat` or integrate your own SSE consumer for `/api/chat/stream`.
- See: `docs/api/chat_stream.md` for curl examples and client helper notes.

### Finishers & Continuations
- When the model output looks incomplete (short text without terminal punctuation or a dangling code fence), the server auto-requests short continuations to complete the response.
- Defaults:
  - Env: `FRONTEND_CONT_ATTEMPTS` defaults to `2` when unset. Set to `0` to disable.
  - Env: `FRONTEND_CONT_TOKENS` controls the size of each continuation chunk.
  - UI: “Continue attempts” defaults to `2` and persists to localStorage; can be set per-conversation.
- Telemetry: continuation attempts are recorded to ContextLog with `act=auto_continue` including `attempt`, `reason` (`short|punct|fence`), and `elapsed_ms`.

See also: docs/ui/diagnostics_drawer.md for the Diagnostics Drawer and how continuation attempts are summarized in the UI.

### Task Suggestions (TGT - Telemetry-Driven Task Generation)

**Status**: ✅ Fully implemented (Weeks 1-9)

**Overview**: Automatically analyzes ContextLog telemetry to generate actionable task cards with priorities, dependencies, and templates.

**UI Access**:
- Open AutonomousPanel → Click "Tasks…" button to view TasksDrawer
- Features: Multi-select, batch approve/dismiss, analytics dashboard, templates

**Core Endpoints** (28 total at `/api/tasks/*`):
- Task Management: `POST /suggest`, `GET /`, `GET /:id`, `POST /:id/approve`, `POST /:id/dismiss`
- Analytics: `GET /analytics`, `GET /funnel`, `GET /stats`
- Templates: `GET /templates`, `POST /from-template/:id`
- Batch Operations: `POST /batch/approve`, `POST /batch/dismiss`
- Dependencies: `GET /dependencies/graph`, `POST /:id/dependencies`
- Prioritization: `POST /reprioritize`, `GET /priority/distribution`
- Scheduler: `GET /scheduler/stats`, `POST /scheduler/run`
- SSE: `GET /stream` - Real-time task updates

**Environment Variables**:

Core Settings:
  - `TASKGEN_ENABLED=1` - Enable TGT system (default: `0`)
  - `TASKGEN_WINDOW_MIN=60` - Analysis window in minutes (default: `60`)
  - `TASKGEN_MIN_CONFIDENCE=0.7` - Minimum confidence threshold (default: `0.7`)
  - `TASKGEN_MAX_TASKS=10` - Maximum tasks to generate (default: `10`)

Analyzers:
  - `TASKGEN_CONT_MIN=5` - Min continuations to trigger (default: `5`)
  - `TASKGEN_CONT_RATIO_THRESHOLD=0.15` - Continuation rate threshold (default: `0.15`)
  - `TASKGEN_UPSTREAM_MIN=3` - Min upstream errors (default: `3`)

Week 8-9 Features:
  - `TASKGEN_AUTO_APPROVE=1` - Enable auto-approval (default: `0`)
  - `TASKGEN_AUTO_APPROVE_CONFIDENCE=0.9` - Min confidence for auto-approve (default: `0.9`)
  - `TASKGEN_AUTO_APPROVE_TRUSTED=error-spike,performance` - Trusted analyzer list

**Features**:
- ✅ 5 Analyzer types: continuation, error-spike, docs-gap, performance, ux-issue
- ✅ Smart priority scoring (Week 9)
- ✅ Auto-approval system (Week 8)
- ✅ Task dependencies with graph visualization (Week 9)
- ✅ Task templates for common patterns (Week 6)
- ✅ Batch operations (Week 6)
- ✅ Funnel analytics with conversion rates (Week 7)
- ✅ Comprehensive analytics dashboard (Week 7)
- ✅ Real-time SSE updates (Week 5)
- ✅ Scheduled background analysis (Week 4)

**UI Components**:
- ✅ TasksDrawer with multi-select and batch actions
- ✅ AnalyticsDashboard with funnel metrics
- ✅ TaskFunnelChart visualization
- ✅ BatchActionBar for bulk operations
- ✅ TemplateSelector for quick task creation
- ✅ PriorityBadge visual indicators
- ✅ DependencyView graph visualization

**Documentation**:
- API Reference: `docs/api/tasks_api.md`
- Implementation: `docs/autonomous/tgt/README.md`
- Integration Guide: `frontend/docs/UI_COMPONENTS_INTEGRATION_GUIDE.md`
- Week Summaries: `docs/autonomous/tgt/TGT_WEEK[1-8]_*.md`

**Testing**: `npm run test` → 22/22 tests passing (Week 8-9 integration)

**Smoke test**: `bash forgekeeper/scripts/test_taskgen.sh 60` (with server running)

### Autonomous Agent & Advanced Learning

**Status**: ✅ Fully implemented (Phase 5 + Diagnostic Reflection)

**Overview**: Autonomous execution with episodic memory, user preference learning, and advanced error recovery capabilities.

**Core Features**:

**1. Episodic Memory with Semantic Search** (Phase 5 Option A)
- ✅ TF-IDF semantic similarity search for past tasks
- ✅ Learn from successful strategies in similar contexts
- ✅ Tool usage suggestions based on historical patterns
- ✅ Iteration complexity estimation from past sessions
- Storage: `.forgekeeper/playground/.episodic_memory.jsonl`
- Lightweight local embeddings (no external dependencies)

**2. User Preference Learning** (Phase 5 Option D)
- ✅ Automatic coding style inference (indentation, quotes, docstrings)
- ✅ Tool choice learning (test frameworks, package managers, formatters)
- ✅ Workflow pattern detection (test location, commit style, branch naming)
- ✅ Documentation style preferences (comment verbosity, README structure)
- Storage: `.forgekeeper/preferences/*.jsonl`
- Confidence-based strengthening through repeated observations

**3. Diagnostic Reflection & Error Recovery** (T313)
- ✅ "5 Whys" root cause analysis for tool failures
- ✅ 14 error categories with specific recovery strategies
- ✅ Automated recovery plan generation and execution
- ✅ Pattern learning from successful recoveries
- ✅ 85-90% recovery success rate for common errors
- Recovery scoring prioritizes automated solutions over user prompts

**Key Endpoints** (20+ at `/api/autonomous/*`, `/api/episodes/*`, `/api/preferences/*`):

Session Management:
- `POST /api/autonomous/start`, `POST /api/autonomous/stop`, `GET /api/autonomous/status`
- `POST /api/autonomous/checkpoint/:session_id`, `POST /api/autonomous/resume/:checkpoint_id`

Diagnostics & Recovery:
- `GET /api/autonomous/diagnose/:session_id` - "5 Whys" failure analysis
- `POST /api/autonomous/recover/:session_id` - Execute recovery plan
- `GET /api/autonomous/sessions/:session_id/errors` - Error history
- `POST /api/autonomous/retry/:session_id` - Retry with diagnostic insights

Episodic Memory:
- `POST /api/episodes/record` - Store successful session
- `POST /api/episodes/search` - Semantic similarity search
- `GET /api/episodes`, `GET /api/episodes/:id`, `DELETE /api/episodes/:id`

User Preferences:
- `POST /api/preferences/infer` - Automatic style inference from code
- `POST /api/preferences/record` - Manual preference recording
- `GET /api/preferences`, `GET /api/preferences/:category`, `PUT /api/preferences/:id`

**Environment Variables**:
- `AUTONOMOUS_ENABLED=1` - Enable autonomous agent (default: `0`)
- `AUTONOMOUS_MAX_ITERATIONS=50` - Max iterations per session (default: `50`)
- `AUTONOMOUS_CHECKPOINT_INTERVAL=10` - Checkpoint every N iterations (default: `10`)
- `AUTONOMOUS_DIAGNOSTIC_ENABLED=1` - Enable diagnostic reflection (default: `1`)
- `EPISODES_MAX_SEARCH_RESULTS=3` - Episodes returned by search (default: `3`)
- `PREFERENCES_MIN_CONFIDENCE=0.6` - Min confidence to apply preference (default: `0.6`)

**Error Categories & Recovery Strategies**:
- `command_not_found` (exit 127) → Install via package manager or alternative tool
- `permission_denied` (EACCES) → Sandbox path retry or chmod fixes
- `file_not_found` (ENOENT) → Path verification and correction
- `timeout` (ETIMEDOUT) → Scope reduction or batch operations
- `tool_not_found` → Decompose to basic tools
- Plus 9 more categories with automated recovery plans

**Performance**:
- Diagnostic reflection: ~1-2 seconds per failure
- Recovery success rate: 85-90% for common errors
- Iteration reduction: 40-60% fewer stuck iterations
- Semantic search: <100ms for TF-IDF similarity (local)

**Documentation**:
- API Reference: `docs/api/autonomous_api.md`
- Episodic Memory: `docs/PHASE5_EPISODIC_MEMORY.md`
- User Preferences: `docs/PHASE5_USER_PREFERENCE_LEARNING.md`
- Diagnostic Reflection: `docs/adr-0003-diagnostic-reflection.md`
- Recovery Strategies: `docs/autonomous/recovery-strategies.md`
- Examples: `docs/autonomous/diagnostic-reflection-examples.md`

**Testing**:
- Diagnostic Reflection: `tests/autonomous/test_diagnostic_reflection.mjs`
- Recovery Scenarios: `tests/autonomous/test_recovery_scenarios.mjs`
- Episodic Memory: `tests/autonomous/test_episodic_memory.mjs`
- Preferences: `tests/autonomous/test_preference_inference.mjs`

### Safe Auto‑PR Loop (SAPL) — Demo
- Preview (dry‑run): `AUTO_PR_ENABLED=1 FRONTEND_PORT=3000 bash forgekeeper/scripts/sapl_demo.sh README.md docs`
  - Calls `POST /api/auto_pr/preview` to validate the allowlist and show unified diff append previews.
  - No PR is created; enable `AUTO_PR_DRYRUN=0` and use the UI’s “Create PR” to open a PR.
- Flags: `AUTO_PR_ENABLED`, `AUTO_PR_DRYRUN`, `AUTO_PR_ALLOW`, `AUTO_PR_LABELS`, `AUTO_PR_AUTOMERGE`.
- PR creation prerequisites:
  - `gh` is installed in the container image (we install it automatically).
  - Provide a GitHub token as `GH_TOKEN` or `GITHUB_TOKEN` (PAT with `repo` scope is sufficient). In Docker Compose, the frontend service forwards these env vars.
  - Ensure `origin` points to your GitHub repo and that Git is configured to use HTTPS (the server route runs `gh auth setup-git` when a token is present).

### GitHub Auth Options (local dev)
- Host env pass‑through (recommended):
  - Windows PowerShell: `$env:GH_TOKEN='ghp_…'; docker compose up -d frontend`
  - Linux/macOS: `export GH_TOKEN='ghp_…'; docker compose up -d frontend`
- Paste token via API (no host env):
  - Enable: `FRONTEND_ENABLE_AUTH_LOCAL=1`
  - `curl -H 'Content-Type: application/json' -d '{"token":"ghp_…"}' http://localhost:${FRONTEND_PORT}/api/auth/github/token`
  - Stored at `.forgekeeper/secrets/gh_token` (gitignored). The server attempts `gh auth login` + `gh auth setup-git` automatically.
- Notes:
  - Tokens are never logged; secrets live under `.forgekeeper/secrets/` which is gitignored by `forgekeeper/.gitignore`.
  - For OAuth/web login (device flow), register an OAuth app and wire its client ID to a future auth handler (tracked as a follow‑up task).

### Upstream Resilience (stub)
- Optional circuit‑breaker/backoff stub for upstream 5xx spikes.
- Flags:
  - `UPSTREAM_CB_ENABLED=1` to enable
  - `UPSTREAM_CB_THRESHOLD` (default 3 failures within window)
  - `UPSTREAM_CB_WINDOW_MS` (default 30000)
  - `UPSTREAM_CB_OPEN_MS` (default 20000)
- Behavior: when open, `/api/chat` and `/api/chat/stream` return `503 { error: "circuit_open", retry_after_ms }` and set `Retry-After`.
- Diagnostics: `GET /api/diagnose` includes `circuit` status.

### Metrics‑Informed Prompting (MIP)
- When `PROMPTING_HINTS_ENABLED=1`, the server may inject a short developer note into the final turn based on recent continuation reasons (e.g., close code fences, finish sentences).
- Tuning envs: `PROMPTING_HINTS_MINUTES` (default 10), `PROMPTING_HINTS_THRESHOLD` (default 0.15).

### Safe Auto‑PR Loop (SAPL)
- Preview: `POST /api/auto_pr/preview` validates an allowlist for docs/config/tests; disabled unless `AUTO_PR_ENABLED=1`.
- Create: `POST /api/auto_pr/create` creates a branch, commits allowlisted files, pushes, and opens a PR via `gh` when `AUTO_PR_ENABLED=1` and `AUTO_PR_DRYRUN=0`.
- Flags: `AUTO_PR_ENABLED`, `AUTO_PR_DRYRUN`, `AUTO_PR_ALLOW`, `AUTO_PR_AUTOMERGE`.
- UI: open Tasks… → Propose PR → Preview → Create PR (enabled by flags).
- See docs/ui/sapl.md for details and the UI flow.

- See: `docs/api/chat_stream.md` for curl examples and client helper notes.

### Conversation Controls
- New Conversation: resets transcript and issues a new `conv_id` (see `docs/ui/new_conversation.md`).
- Stop & Revise: aborts current stream and injects a developer note before the last user turn to relaunch (see `docs/ui/stop_and_revise.md`).
- Diagnostics Drawer: shows recent ContextLog events (enable polling to refresh every 5s).

### Status Bar
- Displays Inference (/health|/healthz), Agent (/metrics), GraphQL (stubbed), Queue (N/A). See `docs/ui/status_bar.md`.

### Dockerized UI (Node.js server):
  - Included in default compose via `python -m forgekeeper`.
  - Serve URL: `http://localhost:${FRONTEND_PORT}` (default `http://localhost:5173`).
  - Container serves static UI with an Express server and runtime config at `/config.json`.
  - Built-in reverse proxy maps `/v1`, `/health`, `/healthz` to the Core container (default target `http://llama-core:8000`).
  - Configure via env: `FRONTEND_VLLM_API_BASE` (default `http://llama-core:8000/v1`), `FRONTEND_VLLM_MODEL` (default `core`), `FRONTEND_PORT` (default `5173` via `.env`; compose fallback `3000` if unset).
  - Local server mode (without Docker): `npm --prefix frontend run build && npm --prefix frontend run serve` (serves `/api/chat`).

- Configure endpoints:
  - Default API base: `/v1` (proxied to `http://localhost:8001`).
  - Change via env: set `VLLM_PROXY_TARGET=http://localhost:8001` before `npm run dev`, or in the UI Settings change API base.
  - Default model: `core` (change in UI or set `VITE_VLLM_MODEL`).

- Run end‑to‑end:
  1) Ensure Core: `python -m forgekeeper up-core`
  2) Start UI: `npm --prefix frontend run dev`
  3) Visit `http://localhost:5173` → send prompts; toggle “Show reasoning” to view chain‑of‑thought.

## CLI Reference

- `python -m forgekeeper compose`
  - Ensures stack via `ensure-stack` (builds when `.env`, `docker-compose.yml`, or `frontend/` sources change) and holds FG; Ctrl+C tears down.
  - Stores a fingerprint at `.forgekeeper/stack_fingerprint.json` to skip rebuilds when configs are unchanged.
  - Tries platform start wrapper (`start.ps1`/`start.sh`). If unavailable/fails, falls back to `up-core` and prints a hint.

- `python -m forgekeeper up-core`
  - Ensures the default core (`llama-core`) is up. Set `FK_CORE_GPU=0` to prefer the CPU profile or `FK_CORE_KIND=vllm` to use vLLM instead.

- `python -m forgekeeper ensure-stack [--build] [--include-mongo] [--profile NAME ...] [--compose-file FILE]`
  - Cross‑platform wrapper over `scripts/ensure_stack.ps1|.sh`. Defaults to the `ui` + `inference` profiles (CPU vs GPU inference selected via `FK_CORE_GPU`).

- `python -m forgekeeper chat [--base-url URL] [--model NAME] [--no-stream] -p PROMPT`
  - PowerShell streaming client on Windows; simple non-streaming Python fallback elsewhere.
  - On non-Windows platforms the fallback runner delegates to `scripts/test_harmony_basic.py`, which always issues the "harmony ok" smoke test regardless of `-p` (known limitation).
  - When using the Python CLI, pass `--no-stream` (two hyphens). The PowerShell switch is `-NoStream` only when calling the `.ps1` directly.
  - Tools demo: add `--tools dir` to enable a minimal `list_dir` function; restrict with `--workdir PATH`.

- `python -m forgekeeper switch-core {llama|vllm} [--no-restart]`
  - Updates `.env` (FK_CORE_KIND, FK_CORE_API_BASE) and restarts the appropriate services (UI + selected core). Use `--no-restart` to skip service changes.

## Harmony Docs

- Protocol summary: `docs/harmony_protocol_summary.md`
- Roadmap: `ROADMAP.md`

## Contributing
- Please read `CONTRIBUTING.md` for the Task Cards policy and local/CI enforcement details.
