# Forgekeeper Architecture Map
**Updated**: 2025-12-15
**Status**: Complete architectural overview

---

## ğŸ—ï¸ System Architecture Overview

Forgekeeper uses a **3-tier architecture** with optional Python CLI:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Browser (localhost:5173 dev / localhost:3000 prod)            â”‚
â”‚  React + Vite Frontend UI                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ HTTP/SSE
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend Server (Node.js Express, Port 3000)                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ server.mjs (Main Express app)                            â”‚  â”‚
â”‚  â”‚ server/ (48 modules organized into 7 categories)         â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€ orchestration/  (Chat flow, tool loops)             â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€ agents/         (Multi-agent system)                â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€ conversations/  (Message bus/store)                 â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€ collaborative/  (Human-in-loop, preferences)        â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€ telemetry/      (Logging, metrics)                  â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€ automation/     (Tasks, PR automation)              â”‚  â”‚
â”‚  â”‚  â””â”€â”€ core/           (Tools, guardrails, thought-world)  â”‚  â”‚
â”‚  â”‚ core/agent/ (Autonomous agent - 8 phases)                â”‚  â”‚
â”‚  â”‚ tools/ (50+ tool definitions)                            â”‚  â”‚
â”‚  â”‚ mcp/ (Model Context Protocol integration)                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ OpenAI-compatible API
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Inference Core (Port 8001)                                     â”‚
â”‚  llama.cpp (GPU) OR LocalAI (CPU) OR vLLM (optional)           â”‚
â”‚  Serves: Local LLM inference via OpenAI-compatible API          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ChromaDB (Port 8000) - Optional Vector Store                   â”‚
â”‚  Used by: Consciousness system for memory/reflection            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Python CLI (Optional - Runs on Host)                           â”‚
â”‚  `python -m forgekeeper [command]`                              â”‚
â”‚  Commands: chat, ensure-stack, up-core, switch-core, talk       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ Docker Containers (4 Services)

### 1. **llama-core** (Primary Inference)
**Image**: `ghcr.io/ggml-org/llama.cpp:server-cuda`
**Profile**: `inference` (default)
**Port**: `8001:8080`

**What It Does**:
- Runs llama.cpp C++ inference server with GPU acceleration (cuBLAS)
- Provides OpenAI-compatible `/v1/chat/completions` API
- Loads quantized GGUF models from `./models/` directory

**What's Inside**:
- llama.cpp server binary
- CUDA libraries for GPU acceleration
- Model files (mounted from host)

**Configuration**:
```bash
LLAMA_MODEL_CORE=/models/your-model.gguf
LLAMA_N_CTX=4096           # Context window
LLAMA_N_GPU_LAYERS=-1      # All layers on GPU
```

---

### 2. **llama-core-cpu** (CPU Fallback)
**Image**: `localai/localai:latest`
**Profile**: `inference-cpu` (when no GPU)
**Port**: `8001:8080`

**What It Does**:
- Same as llama-core but uses CPU-only inference (LocalAI)
- Slower but works without NVIDIA GPU

**What's Inside**:
- LocalAI inference engine
- CPU-optimized model loading
- Model files (mounted from host)

**When Used**: Activated via `FK_CORE_KIND=llama` with `inference-cpu` profile

---

### 3. **vllm-core** (Optional High-Performance)
**Image**: `vllm/vllm-openai:latest`
**Profile**: `inference-vllm` (opt-in)
**Port**: `8001:8000`

**What It Does**:
- High-performance inference with vLLM (PagedAttention, continuous batching)
- Faster for concurrent requests
- Requires more VRAM

**What's Inside**:
- vLLM inference engine
- HuggingFace model loading
- CUDA libraries

**When Used**: Set `FK_CORE_KIND=vllm` in `.env`

---

### 4. **frontend** (Main Application Server)
**Image**: `forgekeeper-frontend` (built locally)
**Profile**: `ui` (default)
**Port**: `3000:3000`

**What It Does**:
- Serves React production build (Vite)
- Runs Node.js Express server with all backend logic
- Executes tools (bash, powershell, file operations)
- Manages autonomous agent, memory systems, conversations

**What's Inside** (Comprehensive):

#### **Node.js Runtime**:
- Node.js 20 (slim base image)
- Express web server
- Production npm dependencies

#### **System Tools**:
- PowerShell (`pwsh`) - for `run_powershell` tool
- Bash (`/bin/bash`) - for `run_bash` tool
- Git - for repository operations
- GitHub CLI (`gh`) - for SAPL auto-PR creation

#### **Application Code**:
```
/app/
â”œâ”€â”€ server.mjs              # Main Express server (entry point)
â”œâ”€â”€ server/                 # 48 server modules (NEW ORGANIZATION)
â”‚   â”œâ”€â”€ orchestration/      # 8 files - Chat orchestration
â”‚   â”œâ”€â”€ agents/             # 9 files - Multi-agent system
â”‚   â”œâ”€â”€ conversations/      # 5 files - Message infrastructure
â”‚   â”œâ”€â”€ collaborative/      # 9 files - Human-AI collaboration
â”‚   â”œâ”€â”€ telemetry/          # 5 files - Logging & metrics
â”‚   â”œâ”€â”€ automation/         # 3 files - Task/PR automation
â”‚   â””â”€â”€ core/               # 9 files - Tools, guardrails, utilities
â”œâ”€â”€ core/agent/             # Autonomous agent (8 phases)
â”‚   â”œâ”€â”€ autonomous.mjs      # Main orchestrator
â”‚   â””â”€â”€ orchestrator/       # Modular components (4 modules)
â”œâ”€â”€ tools/                  # 50+ tool definitions
â”œâ”€â”€ mcp/                    # Model Context Protocol integration
â”œâ”€â”€ graphql/                # Apollo Server for consciousness
â”œâ”€â”€ config/                 # Prompt configurations
â””â”€â”€ dist/                   # React production build
```

#### **Mounted Volumes**:
- `./frontend/tools:/app/tools` - Tool definitions (read/write)
- `./:/workspace` - Full repo access for tools
- `./.forgekeeper:/app/.forgekeeper` - Persistent data

#### **Persistent Data** (.forgekeeper/):
```
.forgekeeper/
â”œâ”€â”€ context_log/            # JSONL event logs (ContextLog)
â”œâ”€â”€ playground/             # Episodic memory JSONL
â”œâ”€â”€ preferences/            # User preferences JSONL
â”œâ”€â”€ learning/               # Outcome tracking
â”œâ”€â”€ chromadb/               # Vector embeddings (if consciousness enabled)
â””â”€â”€ conversation_spaces/    # Agent context files
```

---

### 5. **chromadb** (Optional Vector Database)
**Image**: `chromadb/chroma:latest`
**Profile**: `ui` (started with frontend)
**Port**: `8000:8000`

**What It Does**:
- Vector similarity search for consciousness system
- Stores embeddings for memory retrieval

**What's Inside**:
- ChromaDB vector database
- Persistent storage in `.forgekeeper/chromadb/`

**When Used**: Only if `CONSCIOUSNESS_ENABLED=1` (currently disabled by default)

---

## ğŸ Python Backend (Optional CLI)

**Location**: `forgekeeper/` directory (81 Python files)
**Runs**: On host machine, **NOT in Docker**

**What It Is**:
- Legacy CLI wrapper for convenience
- Orchestrates Docker Compose commands
- Provides alternative chat interface

**Structure**:
```
forgekeeper/
â”œâ”€â”€ __main__.py             # CLI entry point
â”œâ”€â”€ cli/                    # Command handlers
â”‚   â”œâ”€â”€ commands.py         # chat, ensure-stack, etc.
â”‚   â”œâ”€â”€ handlers.py         # Request handling
â”‚   â”œâ”€â”€ args.py             # Argument parsing
â”‚   â””â”€â”€ output.py           # Output formatting
â”œâ”€â”€ core/                   # Git operations (git committer)
â”œâ”€â”€ services/               # ContextLog reader
â””â”€â”€ (other modules - mostly unused)
```

**Available Commands**:
```bash
python -m forgekeeper chat -p "Hello"       # Send chat message
python -m forgekeeper ensure-stack          # Start Docker stack
python -m forgekeeper up-core               # Start inference only
python -m forgekeeper switch-core llama     # Switch inference backend
python -m forgekeeper talk                  # REPL mode
```

**Important**:
- âš ï¸ **This is mostly a convenience wrapper**
- âš ï¸ **Real backend logic is in Node.js (frontend container)**
- âš ï¸ **Can be completely replaced with direct Docker Compose commands**

---

## ğŸ“‚ What Runs Outside Docker

### 1. **Startup Scripts** (Optional - Can use Python CLI instead)
None found - all operations via Python CLI or Docker Compose

### 2. **Python CLI** (`python -m forgekeeper`)
- **NOT dockerized** - runs on host
- Orchestrates Docker Compose
- Provides CLI convenience

### 3. **Direct Docker Compose** (Recommended)
```bash
# Start everything (inference + UI)
docker compose --profile inference --profile ui up --build

# Start just inference
docker compose --profile inference up -d

# Start UI only (assumes inference running)
docker compose --profile ui up
```

---

## ğŸ” Key Architectural Insights

### âœ… **Backend is Node.js, NOT Python**

**Common Misconception**: "Python backend with Node frontend"

**Reality**:
- **ALL backend logic is in Node.js** (frontend container)
- **Python CLI is just a thin wrapper** for Docker commands
- **No Python API server** - all HTTP endpoints are Express (Node.js)

### âœ… **Single Container for "Backend"**

The `frontend` container is actually:
- Frontend UI (React build in `/app/dist/`)
- Backend API (Express server)
- Tool execution (bash, powershell)
- Autonomous agent
- Memory systems
- Everything except LLM inference

### âœ… **Inference is Separate**

- LLM inference runs in separate container (llama-core/vllm-core)
- Frontend calls inference via OpenAI-compatible API
- Clean separation: inference is pluggable (llama.cpp, LocalAI, vLLM, or external API)

### âœ… **No Database Required**

- All persistence via JSONL files (.forgekeeper/)
- ChromaDB only for optional consciousness feature (disabled by default)
- Filesystem-based storage < 1MB total (very lightweight)

---

## ğŸš€ Startup Flow

### Full Stack Startup
```bash
# Via Python CLI (recommended for convenience)
python -m forgekeeper ensure-stack

# Via Docker Compose directly
docker compose --profile inference --profile ui up --build
```

**What Happens**:
1. **Network**: Creates `forgekeeper-net` external network (if needed)
2. **Inference**: Starts llama-core (GPU) or llama-core-cpu
   - Loads model from `./models/`
   - Exposes OpenAI API on port 8001
3. **ChromaDB**: Starts vector DB on port 8000
4. **Frontend**: Builds and starts Node.js server
   - Builds React app (Vite)
   - Installs PowerShell, Git, GitHub CLI
   - Starts Express server on port 3000
   - Mounts workspace and .forgekeeper volumes
5. **Browser**: Access http://localhost:3000

---

## ğŸ“Š Container Resource Usage

### Typical Setup (GPU Available):
```
Container       CPU    RAM      GPU RAM    Purpose
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
llama-core      10%    2 GB     4-8 GB     LLM inference
frontend        5%     512 MB   -          Backend + UI
chromadb        1%     100 MB   -          Vector store
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL           ~16%   ~2.6 GB  4-8 GB
```

### CPU-Only Setup:
```
Container           CPU    RAM      Purpose
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
llama-core-cpu      60%    4 GB     CPU inference
frontend            5%     512 MB   Backend + UI
chromadb            1%     100 MB   Vector store
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL               ~66%   ~4.6 GB
```

---

## ğŸ”§ What's NOT in Docker

1. **Python CLI** (`forgekeeper/` directory)
   - Runs on host
   - Can be replaced with direct `docker compose` commands

2. **Model Files** (`./models/`)
   - Stored on host filesystem
   - Mounted into llama-core container

3. **Workspace** (`./` repo root)
   - Mounted into frontend container as `/workspace`
   - Tools can read/write repo files

4. **Persistent Data** (`.forgekeeper/`)
   - Mounted into frontend container
   - Survives container restarts

---

## ğŸ¯ Summary

### Docker Containers (4-5 active):
1. âœ… **llama-core** - LLM inference (GPU)
2. âœ… **frontend** - Node.js backend + React UI (**ALL THE MAGIC**)
3. âœ… **chromadb** - Vector DB (optional)
4. â¸ï¸ **llama-core-cpu** - CPU fallback (profile: inference-cpu)
5. â¸ï¸ **vllm-core** - High-perf inference (profile: inference-vllm)

### Outside Docker:
- ğŸ Python CLI (optional convenience wrapper)
- ğŸ“ Model files (./models/)
- ğŸ“ Workspace (repo root)
- ğŸ“ Persistent data (.forgekeeper/)

### Backend Reality:
**Backend = Node.js Express in `frontend` container**
- Not Python (Python CLI is just Docker orchestration)
- Single container with Express server + React build
- 48 server modules + autonomous agent + tools
- File-based persistence (no database needed)

---

**The "frontend" container is actually the entire backend!**
