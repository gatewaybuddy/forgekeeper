# Inference node example (vLLM/TRT-LLM)
# Adjust to your environment or container config
VLLM_PORT_CORE=8001
VLLM_MODEL_CORE=./models/mistral-nemo-instruct

