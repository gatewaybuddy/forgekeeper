# Example environment configuration for Docker-based development

# --- Ports ---
FRONTEND_PORT=5173
BACKEND_PORT=8000
PYTHON_PORT=5000

# --- Frontend runtime (dev) ---
FRONTEND_BACKEND_URL=http://backend:8000
VITE_BACKEND_URL=http://localhost:8000

# --- vLLM Core (OpenAI-compatible) ---
# Prefer the official vLLM image unless you maintain a custom build.
VLLM_DOCKER_IMAGE=vllm/vllm-openai:latest
VLLM_PORT_CORE=8001
VLLM_CONTAINER_PORT=8000
VLLM_MODEL_CORE=/models/gpt-oss-20b
VLLM_TP=1
VLLM_DTYPE=bfloat16
VLLM_MAX_MODEL_LEN=32768
VLLM_GPU_MEMORY_UTILIZATION=0.9
VLLM_MAX_NUM_BATCHED_TOKENS=4096

# Host directories mounted into the vLLM containers
VLLM_HF_CACHE_HOST_DIR=./volumes/vllm-cache
VLLM_MODELS_HOST_DIR=./models
VLLM_LOGS_HOST_DIR=./volumes/vllm-logs

# Optional: Hugging Face token for gated models
# HUGGING_FACE_HUB_TOKEN=hf_...

# Service discovery
VLLM_HOST_CORE=vllm-core
FK_CORE_API_BASE=http://vllm-core:8000

# --- Frontend Node server runtime ---
# Upstream OpenAI-compatible base the UI server proxies to.
FRONTEND_VLLM_API_BASE=http://vllm-core:8000/v1
FRONTEND_VLLM_MODEL=core

# Tooling guardrails (server.mjs)
# Comma-separated list of allowed tool names (default: all in registry)
# TOOL_ALLOW=get_time,echo,read_dir,read_file
# Rate limit for /api/chat (requests per minute per IP). 0 disables limiting.
API_RATE_PER_MIN=60

# --- Optional inference gateway (disabled by default) ---
FGK_USE_INFERENCE=0
FGK_INFER_URL=http://localhost:8080
FGK_INFER_KEY=dev-key

# --- Database (future backend) ---
MONGO_URI=mongodb://localhost:27017/forgekeeper?directConnection=true&retryWrites=false

# --- OpenAI key (only when calling external OpenAI) ---
# OPENAI_API_KEY=sk-...
