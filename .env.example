# Example environment configuration for Docker-based development
#
# CAPABILITY-FIRST PHILOSOPHY (T301-T306):
# Forgekeeper defaults to MAXIMUM CAPABILITY with all guardrails OPTIONAL.
# Choose your security layer based on environment:
#
# Layer 1 - MAXIMUM CAPABILITY (Default):
#   ENABLE_FS_SANDBOX=0           # Full filesystem access
#   ENABLE_LOG_REDACTION=0        # No redaction (full transparency)
#   RATE_LIMIT_ENABLED=0          # No rate limits
#   RESOURCE_QUOTAS_ENABLED=0     # No resource quotas
#   → Recommended for: Local development, maximum iteration speed
#
# Layer 2 - TEAM ENVIRONMENT:
#   ENABLE_FS_SANDBOX=0           # Still no sandbox
#   ENABLE_LOG_REDACTION=1        # Redact in shared logs
#   REDACTION_MODE=minimal        # Only critical secrets
#   RATE_LIMIT_ENABLED=0          # No limits (trust team)
#   RESOURCE_QUOTAS_ENABLED=0     # No quotas
#   → Recommended for: Shared dev environments, team collaboration
#
# Layer 3 - PRODUCTION (Compliance):
#   ENABLE_FS_SANDBOX=1           # Sandbox for safety
#   ENABLE_LOG_REDACTION=1        # Full redaction
#   REDACTION_MODE=aggressive     # Maximum redaction
#   REDACTION_CONTEXT=production  # Production rules
#   RATE_LIMIT_ENABLED=1          # Rate limiting
#   RESOURCE_QUOTAS_ENABLED=1     # Resource quotas
#   → Recommended for: Production deployments, compliance requirements

# Core selection (default: llama). Options: llama, vllm
FK_CORE_KIND=llama

# --- Ports ---
FRONTEND_PORT=5173
BACKEND_PORT=8000
PYTHON_PORT=5000

# --- Frontend runtime (dev) ---
FRONTEND_BACKEND_URL=http://backend:8000
VITE_BACKEND_URL=http://localhost:8000

# --- llama.cpp Core (OpenAI-compatible default) ---
# Uses the official llama.cpp server image with OpenAI API enabled via --api
LLAMA_DOCKER_IMAGE=ghcr.io/ggerganov/llama.cpp:server
LLAMA_PORT_CORE=8001
LLAMA_CONTAINER_PORT=8000
LLAMA_MODEL_CORE=/models/your-model.gguf
LLAMA_N_CTX=8192
# Optional rope scaling to enable >8k contexts (use with care)
LLAMA_ROPE_SCALING_TYPE=none      # options: none, linear, yarn
LLAMA_ROPE_FREQ_BASE=10000.0
LLAMA_ROPE_FREQ_SCALE=1.0
LLAMA_N_GPU_LAYERS=0

# Host directories mounted into the llama.cpp container
LLAMA_MODELS_HOST_DIR=./models

# Optional: Hugging Face token for gated models
# HUGGING_FACE_HUB_TOKEN=hf_...

# Service discovery
FK_CORE_API_BASE=http://llama-core:8000

# --- Frontend Node server runtime ---
# Upstream OpenAI-compatible base the UI server proxies to.
FRONTEND_VLLM_API_BASE=http://llama-core:8000/v1
FRONTEND_VLLM_MODEL=core

# Tooling guardrails (server.mjs)
# Comma-separated list of allowed tool names (default: all in registry)
# TOOL_ALLOW=get_time,echo,read_dir,read_file
# Rate limit for /api/chat (requests per minute per IP). 0 disables limiting.
API_RATE_PER_MIN=60

# Harmony rendering mode
# When set to 1 (default), the server uses a minimal Harmony prompt that only
# pre-fills the final channel. Set to 0 to include the full policy section that
# instructs the model to write private reasoning in the analysis channel; the
# server extracts that as `assistant.reasoning` for the UI.
FRONTEND_USE_HARMONY=1
FRONTEND_HARMONY_MINIMAL=0
FRONTEND_DISABLE_SYSTEM=0

# --- Optional: vLLM Core (kept for compatibility; disabled by default) ---
VLLM_DOCKER_IMAGE=vllm/vllm-openai:latest
VLLM_PORT_CORE=8001
VLLM_CONTAINER_PORT=8000
VLLM_MODEL_CORE=/models/gpt-oss-20b
VLLM_TP=1
VLLM_DTYPE=bfloat16
VLLM_MAX_MODEL_LEN=32768
VLLM_GPU_MEMORY_UTILIZATION=0.9
VLLM_MAX_NUM_BATCHED_TOKENS=4096
VLLM_HF_CACHE_HOST_DIR=./volumes/vllm-cache
VLLM_MODELS_HOST_DIR=./models
VLLM_LOGS_HOST_DIR=./volumes/vllm-logs

# --- Frontend generation budgets ---
# Aim high by default; upstream/model context may cap further.
FRONTEND_MAX_TOKENS=8192
FRONTEND_CONT_TOKENS=4096
# Optional override for server budgeting; otherwise derives from LLAMA_N_CTX/VLLM_MAX_MODEL_LEN
# FRONTEND_CTX_LIMIT=8192

# --- Sampling defaults (lowered for coherence) ---
# Temperature and top_p are applied by the frontend Node server for both
# /v1/chat/completions and Harmony /v1/completions paths.
FRONTEND_TEMP=0.0
FRONTEND_TOP_P=0.4
# Repetition controls (OpenAI-compatible backends)
FRONTEND_PRESENCE_PENALTY=0.0
FRONTEND_FREQUENCY_PENALTY=0.2
# Auto-continue attempts for streamed responses (0 disables continuation)
# Auto-continue attempts for streamed responses (0 disables continuation)
FRONTEND_CONT_ATTEMPTS=2

# --- Optional inference gateway (disabled by default) ---
FGK_USE_INFERENCE=0
FGK_INFER_URL=http://localhost:8080
FGK_INFER_KEY=dev-key

# --- Built-in tool runners (server-side) ---
# Enable shell tools for automation in dev (use with care):
FRONTEND_ENABLE_POWERSHELL=1
FRONTEND_ENABLE_BASH=1
# Default working directories for shell tools (inside container)
# PWSH_CWD=/workspace/sandbox
# BASH_CWD=/workspace/sandbox

# --- T11: Tool Execution Sandbox and Gating ---
# Global toggle for tool execution (1=enabled, 0=disabled)
# When disabled, all tool calls return a gated error with telemetry
TOOLS_EXECUTION_ENABLED=1
# Tool execution timeout in milliseconds (default: 30000 = 30 seconds)
TOOL_TIMEOUT_MS=30000
# Maximum retries for failed tool executions (default: 0 = no retries)
TOOL_MAX_RETRIES=0
# Maximum output size in bytes (default: 1048576 = 1MB)
# Outputs exceeding this will be truncated with a warning
TOOL_MAX_OUTPUT_BYTES=1048576

# --- T301: Filesystem Sandbox (Optional) ---
# Enable filesystem sandbox (0=disabled [DEFAULT], 1=enabled)
# When disabled: Full filesystem access for maximum capability
# When enabled: Tools are restricted to TOOLS_FS_ROOT directory
ENABLE_FS_SANDBOX=0

# Filesystem root for tool operations
# When ENABLE_FS_SANDBOX=0: This is the base directory (default: /workspace)
# When ENABLE_FS_SANDBOX=1: Tools are sandboxed to this directory only
TOOLS_FS_ROOT=/workspace

# --- Repository Write Access (write_repo_file tool) ---
# Enable repository file writing (⚠️ USE ONLY IN LOCAL DEV SANDBOX)
FRONTEND_ENABLE_REPO_WRITE=1
# Comma-separated allowlist of file patterns the agent can modify
# Default is very restrictive (only Dockerfile and docker-compose.yml)
# For local development, you can expand this to enable autonomous code updates:
# REPO_WRITE_ALLOW=frontend/Dockerfile,docker-compose.yml,frontend/src/**/*.tsx,frontend/src/**/*.ts,frontend/src/**/*.jsx,frontend/src/**/*.js,frontend/src/**/*.mjs,frontend/src/**/*.css,frontend/tests/**/*.mjs,frontend/tests/**/*.js,frontend/tools/**/*.mjs,frontend/core/**/*.mjs,frontend/package.json,docs/**/*.md,README.md,.env.example,forgekeeper/**/*.py,scripts/**/*.sh,scripts/**/*.ps1,.github/**/*.yml,tasks.md

# --- T303: Per-Request Rate Limiting (Optional) ---
# Enable rate limiting for chat endpoints (0=disabled [DEFAULT], 1=enabled)
# When disabled: Unlimited local execution for maximum capability
# When enabled: Token bucket rate limiting applied
RATE_LIMIT_ENABLED=0
# Maximum tokens (burst size) - allows this many requests before throttling (when enabled)
RATE_LIMIT_CAPACITY=100
# Tokens refilled per second (steady-state throughput) (when enabled)
RATE_LIMIT_REFILL_RATE=10
# Tokens consumed per request (when enabled)
RATE_LIMIT_COST_PER_REQUEST=1

# --- T302: Log Redaction (Optional) ---
# Enable sensitive data redaction in logs (0=disabled [DEFAULT], 1=enabled)
# When disabled: Full logging transparency for debugging (recommended for local dev)
# When enabled: Sensitive data (API keys, passwords, etc.) redacted from logs
ENABLE_LOG_REDACTION=0

# Redaction mode (when ENABLE_LOG_REDACTION=1):
#   - off: No redaction (same as ENABLE_LOG_REDACTION=0)
#   - minimal: Redact only critical secrets (API keys, passwords)
#   - standard: Redact common sensitive data
#   - aggressive: Redact all potentially sensitive data including emails, phones, long strings
REDACTION_MODE=off

# Redaction context (when ENABLE_LOG_REDACTION=1):
#   - dev: Minimal redaction, maximum transparency
#   - staging: Standard redaction
#   - production: Aggressive redaction for compliance
REDACTION_CONTEXT=dev

# --- T304: Resource Quotas (Optional) ---
# Enable per-tool resource quotas (0=disabled [DEFAULT], 1=enabled)
# When disabled: No limits on disk, memory, or request rates
# When enabled: Enforce quotas to prevent resource exhaustion
RESOURCE_QUOTAS_ENABLED=0
# Rate limit per tool (requests per minute) (when enabled)
TOOL_RATE_LIMIT_PER_MIN=30
# Disk quota per tool in bytes (when enabled)
TOOL_DISK_QUOTA_BYTES=10485760
# Memory limit per tool in MB (when enabled)
TOOL_MEMORY_LIMIT_MB=512
# CPU timeout per tool in milliseconds (when enabled)
TOOL_CPU_TIMEOUT_MS=30000

# --- Database (future backend) ---
MONGO_URI=mongodb://localhost:27017/forgekeeper?directConnection=true&retryWrites=false

# --- OpenAI key (only when calling external OpenAI) ---
# OPENAI_API_KEY=sk-...

# --- Self-Review Iteration (M2: Quality Improvement) ---
# Enable iterative self-review to improve response quality
FRONTEND_ENABLE_REVIEW=0
# Number of review passes (1-5, default 3)
FRONTEND_REVIEW_ITERATIONS=3
# Quality threshold to accept response (0.0-1.0, default 0.7)
FRONTEND_REVIEW_THRESHOLD=0.7
# Maximum regeneration attempts per review cycle (default 2)
FRONTEND_REVIEW_MAX_REGENERATIONS=2
# Token budget for review evaluation (default 512)
FRONTEND_REVIEW_EVAL_TOKENS=512
# Review mode controls when review is triggered:
#   - always: Review all responses (may slow down tool execution)
#   - never: Disable review entirely
#   - on_error: Only review when errors occur
#   - on_incomplete: Only review incomplete responses
#   - on_complex: Only review text-only responses, skip when tools are used (RECOMMENDED)
# Default: on_complex (prevents review loop deadlock for tool-heavy requests)
FRONTEND_REVIEW_MODE=on_complex

# --- Chunked Reasoning (M2: Overcome Context Limits) ---
# Enable chunked response generation for comprehensive outputs
FRONTEND_ENABLE_CHUNKED=0
# Maximum chunks per response (2-10, default 5)
FRONTEND_CHUNKED_MAX_CHUNKS=5
# Target tokens per chunk (default 1024)
FRONTEND_CHUNKED_TOKENS_PER_CHUNK=1024
# Minimum tokens to trigger auto-chunking (default 2048)
FRONTEND_CHUNKED_AUTO_THRESHOLD=2048
# Auto-outline: let model determine chunk count (1) or use fixed count (0)
FRONTEND_CHUNKED_AUTO_OUTLINE=1
# Maximum outline generation attempts (default 2)
FRONTEND_CHUNKED_OUTLINE_RETRIES=2
# Token budget for outline generation (default 512)
FRONTEND_CHUNKED_OUTLINE_TOKENS=512
# Review each chunk individually (0) or only final assembly (default 0)
FRONTEND_CHUNKED_REVIEW_PER_CHUNK=0

# --- Combined Mode (Review + Chunked) ---
# Strategy when both features enabled: per_chunk, final_only, both (default: final_only)
FRONTEND_COMBINED_REVIEW_STRATEGY=final_only
# Auto-detection: enable review mode automatically based on heuristics (default 0)
FRONTEND_AUTO_REVIEW=0
# Auto-detection: enable chunked mode automatically based on heuristics (default 0)
FRONTEND_AUTO_CHUNKED=0

# --- Sprint 6: Two-Phase Mode & Reflection Pass ---
# Two-Phase Harmony Mode: Separate analysis from execution for better control
# Phase 1: Generate detailed plan, halt for user review
# Phase 2: Execute based on approved (optionally edited) plan
FRONTEND_ENABLE_TWO_PHASE=0
# Auto-detect when to use two-phase mode based on request heuristics (default 0)
FRONTEND_AUTO_TWO_PHASE=0
# Confidence threshold for auto-detection (0.0-1.0, default 0.6)
FRONTEND_AUTO_TWO_PHASE_THRESHOLD=0.6
# Token budget for Phase 1 analysis generation (default 4096)
FRONTEND_TWO_PHASE_ANALYSIS_TOKENS=4096
# Token budget for Phase 2 execution (default 8192)
FRONTEND_TWO_PHASE_EXECUTION_TOKENS=8192
# Allow user to edit plan before Phase 2 (0=no, 1=yes, default 1)
FRONTEND_TWO_PHASE_ALLOW_EDIT=1

# Reflection Pass: Post-generation self-critique and correction
# After generating response, run quick self-critique and apply small corrections
FRONTEND_REFLECTION_ENABLED=0
# Token budget for reflection critique (default 256)
FRONTEND_REFLECTION_CRITIQUE_TOKENS=256
# Token budget for applying corrections (default 512)
FRONTEND_REFLECTION_CORRECTION_TOKENS=512
# Minimum confidence to apply corrections (0.0-1.0, default 0.6)
FRONTEND_REFLECTION_MIN_CONFIDENCE=0.6
# Maximum reflection iterations (default 1)
FRONTEND_REFLECTION_MAX_ITER=1

# --- Phase 4: Autonomous Agent Mode ---
# Enable autonomous agent mode (default: 0)
FRONTEND_ENABLE_AUTONOMOUS_MODE=1
# Maximum iterations for autonomous mode (default: 15)
AUTONOMOUS_MAX_ITERATIONS=15
# Checkpoint interval - iterations between progress updates (default: 5)
AUTONOMOUS_CHECKPOINT_INTERVAL=5
# Error threshold before stopping (default: 3)
AUTONOMOUS_ERROR_THRESHOLD=3
# Playground/sandbox directory for autonomous work (default: .forgekeeper/playground)
AUTONOMOUS_PLAYGROUND_ROOT=.forgekeeper/playground

# --- T401-T409: Model Context Protocol (MCP) Integration ---
# Enable MCP server integration (default: 1)
# Set to 0 to disable MCP entirely
MCP_ENABLED=1

# Path to MCP servers configuration file (default: .forgekeeper/mcp-servers.json)
# This file defines which MCP servers to load and their settings
MCP_SERVERS_CONFIG=.forgekeeper/mcp-servers.json

# Auto-reload MCP servers when config changes (default: 1)
# Set to 0 to disable hot-reload
MCP_AUTO_RELOAD=1

# Health check interval for MCP servers in milliseconds (default: 60000 = 1 minute)
# MCP servers will be restarted automatically if health checks fail
MCP_HEALTH_CHECK_INTERVAL=60000

# Example MCP servers (configure in mcp-servers.json):
# - @modelcontextprotocol/server-github: GitHub API integration
# - @modelcontextprotocol/server-postgres: PostgreSQL database access
# - @modelcontextprotocol/server-filesystem: Filesystem operations
# - @modelcontextprotocol/server-git: Git repository operations
# - @modelcontextprotocol/server-slack: Slack workspace integration
# - @modelcontextprotocol/server-puppeteer: Browser automation
#
# See .forgekeeper/mcp-servers.example.json for configuration examples

# --- T308-T312: Collaborative Intelligence (Human-in-the-Loop) ---
# Phase 8: Enable human-in-the-loop collaboration with approval workflows,
# decision checkpoints, feedback collection, preference learning, and adaptive recommendations

# Core Settings
# Enable/disable collaborative intelligence (default: 0 - disabled for backward compatibility)
AUTONOMOUS_ENABLE_COLLABORATION=0

# Feedback and learning (default: 1 - enabled)
AUTONOMOUS_ENABLE_FEEDBACK=1
AUTONOMOUS_FEEDBACK_LEARNING=1

# Approval System (T301-T303)
# Timeout for approval requests in milliseconds (default: 300000 = 5 minutes)
AUTONOMOUS_APPROVAL_TIMEOUT_MS=300000
# Minimum risk level requiring approval: low, medium, high, critical (default: high)
AUTONOMOUS_APPROVAL_REQUIRED=high

# Decision Checkpoints (T304-T306)
# Confidence threshold for triggering checkpoints (0.0-1.0, default: 0.7)
AUTONOMOUS_CHECKPOINT_THRESHOLD=0.7
# Timeout for checkpoint decisions in milliseconds (default: 600000 = 10 minutes)
AUTONOMOUS_CHECKPOINT_TIMEOUT_MS=600000

# Feedback Collection (T307)
# Require rating with feedback (0=optional, 1=required, default: 0)
AUTONOMOUS_REQUIRE_FEEDBACK_RATING=0
# Maximum feedback entries to store in memory (default: 5000)
AUTONOMOUS_MAX_FEEDBACK_ENTRIES=5000

# Preference Analysis (T308)
# Minimum samples required to detect a pattern (default: 5)
PREFERENCE_MIN_SAMPLES=5
# Confidence threshold for using patterns (0.0-1.0, default: 0.6)
PREFERENCE_CONFIDENCE_THRESHOLD=0.6
# Frequency threshold for pattern detection (0.0-1.0, default: 0.5 = 50%)
PREFERENCE_FREQUENCY_THRESHOLD=0.5
# Maximum patterns to track per category (default: 10)
PREFERENCE_MAX_PATTERNS_PER_CATEGORY=10

# Adaptive Recommendations (T309)
# Use preferences in recommendations (0=disabled, 1=enabled, default: 1)
RECOMMENDATION_USE_PREFERENCES=1
# Confidence boost for preferred options (0.0-1.0, default: 0.15 = +15%)
RECOMMENDATION_CONFIDENCE_BOOST=0.15
# Weight of historical choices in scoring (0.0-1.0, default: 0.3 = 30%)
RECOMMENDATION_HISTORY_WEIGHT=0.3
# Minimum confidence to make a recommendation (0.0-1.0, default: 0.4)
RECOMMENDATION_MIN_CONFIDENCE=0.4
# Enable A/B testing for recommendation strategies (0=disabled, 1=enabled, default: 0)
RECOMMENDATION_AB_TESTING=0
# Maximum recommendation history size (default: 1000)
RECOMMENDATION_MAX_HISTORY=1000

# Configuration Examples:
#
# Development (permissive):
#   AUTONOMOUS_ENABLE_COLLABORATION=1
#   AUTONOMOUS_APPROVAL_REQUIRED=critical  # Only critical ops need approval
#   PREFERENCE_MIN_SAMPLES=3               # Lower threshold for faster learning
#
# Production (strict):
#   AUTONOMOUS_ENABLE_COLLABORATION=1
#   AUTONOMOUS_APPROVAL_REQUIRED=medium    # Most ops need approval
#   AUTONOMOUS_REQUIRE_FEEDBACK_RATING=1   # Require ratings
#   PREFERENCE_MIN_SAMPLES=10              # Higher threshold for reliability
#   PREFERENCE_CONFIDENCE_THRESHOLD=0.75   # Higher confidence required
#   RECOMMENDATION_MIN_CONFIDENCE=0.6      # Higher bar for recommendations
#
# Testing (disabled):
#   AUTONOMOUS_ENABLE_COLLABORATION=0      # Disable for unit tests
#   AUTONOMOUS_FEEDBACK_LEARNING=0

# Documentation: docs/autonomous/collaborative-intelligence.md
# ADR: docs/adr/adr-0003-collaborative-intelligence.md

