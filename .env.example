# Example environment configuration for Docker-based development

# Service ports
FRONTEND_PORT=3000
# BACKEND_PORT drives the backend Docker build and runtime. `docker compose up backend`
# rebuilds the image when backend code changes and forwards this port into the
# container while also exporting `PORT` for the Node process.
BACKEND_PORT=8000
PYTHON_PORT=5000

# Frontend runtime + dev configuration
FRONTEND_BACKEND_URL=http://backend:8000
VITE_BACKEND_URL=http://localhost:8000

# LLM backend
LLM_BACKEND=vllm
# To use the llama.cpp backend instead, uncomment the line below and provide model settings.
# LLM_BACKEND=llama_cpp
# FK_MODEL_PATH=./models/ggml-model.gguf
# FK_THREADS=4
# FK_GPU_LAYERS=0

# Tiny model for fast CPU-only tests (development only)
# Set USE_TINY_MODEL=true to force the bundled GPT-2 on CPU, overriding FK_MODEL_PATH/FK_LLM_IMPL.
# Unset USE_TINY_MODEL or set it to false to use production models.
# USE_TINY_MODEL=true
# FK_DEVICE=cpu
# FK_DTYPE=float32

# Location for downloaded models
MODEL_DIR=./models

# vLLM service ports
VLLM_PORT_CORE=8001
VLLM_PORT_CODER=8002
# Container port exposed by the official image. Override only if you pass
# --port to the vLLM entrypoint.
VLLM_CONTAINER_PORT=8000

# vLLM model settings (default to oss-gpt-20b)
VLLM_MODEL_CORE=oss-gpt-20b
VLLM_MODEL_CODER=oss-gpt-20b
VLLM_TP=1
VLLM_MAX_MODEL_LEN=4096
VLLM_GPU_MEMORY_UTILIZATION=0.9

# Container image for the vLLM runtime. Stick with the official image to stay
# aligned with https://docs.vllm.ai/en/latest/deployment/docker.html
VLLM_DOCKER_IMAGE=vllm/vllm-openai:latest

# Host directories mounted into the vLLM containers for cache, optional local
# models, and logs. Override with absolute paths to place them on external
# storage.
VLLM_HF_CACHE_HOST_DIR=./volumes/vllm-cache
VLLM_MODELS_HOST_DIR=./volumes/vllm-models
VLLM_LOGS_HOST_DIR=./volumes/vllm-logs

# Optional override for where the vLLM launch scripts write logs inside the
# container/host environment. Defaults to /var/log/vllm in Docker.
# VLLM_LOG_DIR=/var/log/vllm

# Optional Hugging Face access token forwarded into the containers. Required
# for gated models when using the official vLLM image.
# HUGGING_FACE_HUB_TOKEN=hf_...

# Service discovery inside Docker network
VLLM_HOST_CORE=vllm-core
VLLM_HOST_CODER=vllm-coder
FK_CORE_API_BASE=http://vllm-core:8000/v1
FK_CODER_API_BASE=http://vllm-coder:8000/v1

## Inference Gateway (OpenAI-compatible) routing
# Enabled by default. Routes all LLM calls via the local gateway
# and overrides FK_*_API_BASE accordingly.
FGK_USE_INFERENCE=1
FGK_INFER_URL=http://localhost:8080
FGK_INFER_KEY=dev-key

# Database connection
MONGO_URI=mongodb://localhost:27017/forgekeeper?directConnection=true&retryWrites=false

# Uncomment and set if accessing OpenAI services
# OPENAI_API_KEY=sk-...
 # Optional: default OpenAI model when using openai backend in v2 CLI
 # FK_OPENAI_MODEL=gpt-4o-mini

## TritonLLM (OpenAI-compatible) configuration for v2 CLI
# TRITON_URL=http://localhost:8000
# TRITON_MODEL=oss-20b

## v2 stabilization flags (env kill switches)
# Primary inference backend selection for the agent path
# Options: triton | llamacpp | transformers
FGK_INFERENCE_BACKEND=triton

# Bypass the inference gateway during stabilization
# 0 = direct to backend; 1 = via gateway
FGK_USE_GATEWAY=0

# Memory backend selection for agent memory
# Options: kv | vector (vector later)
FGK_MEMORY_BACKEND=kv
