# Example environment configuration for Docker-based development

# Core selection (default: llama). Options: llama, vllm
FK_CORE_KIND=llama

# --- Ports ---
FRONTEND_PORT=5173
BACKEND_PORT=8000
PYTHON_PORT=5000

# --- Frontend runtime (dev) ---
FRONTEND_BACKEND_URL=http://backend:8000
VITE_BACKEND_URL=http://localhost:8000

# --- llama.cpp Core (OpenAI-compatible default) ---
# Uses the official llama.cpp server image with OpenAI API enabled via --api
LLAMA_DOCKER_IMAGE=ghcr.io/ggerganov/llama.cpp:server
LLAMA_PORT_CORE=8001
LLAMA_CONTAINER_PORT=8000
LLAMA_MODEL_CORE=/models/your-model.gguf
LLAMA_N_CTX=8192
# Optional rope scaling to enable >8k contexts (use with care)
LLAMA_ROPE_SCALING_TYPE=none      # options: none, linear, yarn
LLAMA_ROPE_FREQ_BASE=10000.0
LLAMA_ROPE_FREQ_SCALE=1.0
LLAMA_N_GPU_LAYERS=0

# Host directories mounted into the llama.cpp container
LLAMA_MODELS_HOST_DIR=./models

# Optional: Hugging Face token for gated models
# HUGGING_FACE_HUB_TOKEN=hf_...

# Service discovery
FK_CORE_API_BASE=http://llama-core:8000

# --- Frontend Node server runtime ---
# Upstream OpenAI-compatible base the UI server proxies to.
FRONTEND_VLLM_API_BASE=http://llama-core:8000/v1
FRONTEND_VLLM_MODEL=core

# Tooling guardrails (server.mjs)
# Comma-separated list of allowed tool names (default: all in registry)
# TOOL_ALLOW=get_time,echo,read_dir,read_file
# Rate limit for /api/chat (requests per minute per IP). 0 disables limiting.
API_RATE_PER_MIN=60

# Harmony rendering mode
# When set to 1 (default), the server uses a minimal Harmony prompt that only
# pre-fills the final channel. Set to 0 to include the full policy section that
# instructs the model to write private reasoning in the analysis channel; the
# server extracts that as `assistant.reasoning` for the UI.
FRONTEND_USE_HARMONY=1
FRONTEND_HARMONY_MINIMAL=0
FRONTEND_DISABLE_SYSTEM=0

# --- Optional: vLLM Core (kept for compatibility; disabled by default) ---
VLLM_DOCKER_IMAGE=vllm/vllm-openai:latest
VLLM_PORT_CORE=8001
VLLM_CONTAINER_PORT=8000
VLLM_MODEL_CORE=/models/gpt-oss-20b
VLLM_TP=1
VLLM_DTYPE=bfloat16
VLLM_MAX_MODEL_LEN=32768
VLLM_GPU_MEMORY_UTILIZATION=0.9
VLLM_MAX_NUM_BATCHED_TOKENS=4096
VLLM_HF_CACHE_HOST_DIR=./volumes/vllm-cache
VLLM_MODELS_HOST_DIR=./models
VLLM_LOGS_HOST_DIR=./volumes/vllm-logs

# --- Frontend generation budgets ---
# Aim high by default; upstream/model context may cap further.
FRONTEND_MAX_TOKENS=8192
FRONTEND_CONT_TOKENS=4096
# Optional override for server budgeting; otherwise derives from LLAMA_N_CTX/VLLM_MAX_MODEL_LEN
# FRONTEND_CTX_LIMIT=8192

# --- Sampling defaults (lowered for coherence) ---
# Temperature and top_p are applied by the frontend Node server for both
# /v1/chat/completions and Harmony /v1/completions paths.
FRONTEND_TEMP=0.0
FRONTEND_TOP_P=0.4
# Repetition controls (OpenAI-compatible backends)
FRONTEND_PRESENCE_PENALTY=0.0
FRONTEND_FREQUENCY_PENALTY=0.2
# Auto-continue attempts for streamed responses (0 disables continuation)
FRONTEND_CONT_ATTEMPTS=0

# --- Optional inference gateway (disabled by default) ---
FGK_USE_INFERENCE=0
FGK_INFER_URL=http://localhost:8080
FGK_INFER_KEY=dev-key

# --- Database (future backend) ---
MONGO_URI=mongodb://localhost:27017/forgekeeper?directConnection=true&retryWrites=false

# --- OpenAI key (only when calling external OpenAI) ---
# OPENAI_API_KEY=sk-...
