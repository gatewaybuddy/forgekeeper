# Example environment configuration for Docker-based development

# Service ports
FRONTEND_PORT=3000
BACKEND_PORT=8000
PYTHON_PORT=5000

# LLM backend
LLM_BACKEND=vllm

# Tiny model for fast CPU-only tests (development only)
# Set USE_TINY_MODEL=true to bypass vLLM and run the bundled GPT-2 on CPU.
# USE_TINY_MODEL=true
# FK_DEVICE=cpu
# FK_DTYPE=float32

# Location for downloaded models
MODEL_DIR=./models

# vLLM service ports
VLLM_PORT_CORE=8001
VLLM_PORT_CODER=8002

# vLLM model settings
VLLM_MODEL_CORE=mistral-nemo-instruct
VLLM_MODEL_CODER=codellama-13b-python
VLLM_TP=1
VLLM_MAX_MODEL_LEN=4096
VLLM_GPU_MEMORY_UTILIZATION=0.9

# Service discovery inside Docker network
VLLM_HOST_CORE=vllm-core
VLLM_HOST_CODER=vllm-coder
FK_CORE_API_BASE=http://vllm-core:8001
FK_CODER_API_BASE=http://vllm-coder:8002

# Database connection
MONGO_URI=mongodb://localhost:27017/forgekeeper

# Uncomment and set if accessing OpenAI services
# OPENAI_API_KEY=sk-...
