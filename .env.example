# Example environment configuration for Docker-based development

# Core selection (default: llama). Options: llama, vllm
FK_CORE_KIND=llama

# --- Ports ---
FRONTEND_PORT=5173
BACKEND_PORT=8000
PYTHON_PORT=5000

# --- Frontend runtime (dev) ---
FRONTEND_BACKEND_URL=http://backend:8000
VITE_BACKEND_URL=http://localhost:8000

# --- llama.cpp Core (OpenAI-compatible default) ---
# Uses the official llama.cpp server image with OpenAI API enabled via --api
LLAMA_DOCKER_IMAGE=ghcr.io/ggerganov/llama.cpp:server
LLAMA_PORT_CORE=8001
LLAMA_CONTAINER_PORT=8000
LLAMA_MODEL_CORE=/models/your-model.gguf
LLAMA_N_CTX=8192
# Optional rope scaling to enable >8k contexts (use with care)
LLAMA_ROPE_SCALING_TYPE=none      # options: none, linear, yarn
LLAMA_ROPE_FREQ_BASE=10000.0
LLAMA_ROPE_FREQ_SCALE=1.0
LLAMA_N_GPU_LAYERS=0

# Host directories mounted into the llama.cpp container
LLAMA_MODELS_HOST_DIR=./models

# Optional: Hugging Face token for gated models
# HUGGING_FACE_HUB_TOKEN=hf_...

# Service discovery
FK_CORE_API_BASE=http://llama-core:8000

# --- Frontend Node server runtime ---
# Upstream OpenAI-compatible base the UI server proxies to.
FRONTEND_VLLM_API_BASE=http://llama-core:8000/v1
FRONTEND_VLLM_MODEL=core

# Tooling guardrails (server.mjs)
# Comma-separated list of allowed tool names (default: all in registry)
# TOOL_ALLOW=get_time,echo,read_dir,read_file
# Rate limit for /api/chat (requests per minute per IP). 0 disables limiting.
API_RATE_PER_MIN=60

# Harmony rendering mode
# When set to 1 (default), the server uses a minimal Harmony prompt that only
# pre-fills the final channel. Set to 0 to include the full policy section that
# instructs the model to write private reasoning in the analysis channel; the
# server extracts that as `assistant.reasoning` for the UI.
FRONTEND_USE_HARMONY=1
FRONTEND_HARMONY_MINIMAL=0
FRONTEND_DISABLE_SYSTEM=0

# --- Optional: vLLM Core (kept for compatibility; disabled by default) ---
VLLM_DOCKER_IMAGE=vllm/vllm-openai:latest
VLLM_PORT_CORE=8001
VLLM_CONTAINER_PORT=8000
VLLM_MODEL_CORE=/models/gpt-oss-20b
VLLM_TP=1
VLLM_DTYPE=bfloat16
VLLM_MAX_MODEL_LEN=32768
VLLM_GPU_MEMORY_UTILIZATION=0.9
VLLM_MAX_NUM_BATCHED_TOKENS=4096
VLLM_HF_CACHE_HOST_DIR=./volumes/vllm-cache
VLLM_MODELS_HOST_DIR=./models
VLLM_LOGS_HOST_DIR=./volumes/vllm-logs

# --- Frontend generation budgets ---
# Aim high by default; upstream/model context may cap further.
FRONTEND_MAX_TOKENS=8192
FRONTEND_CONT_TOKENS=4096
# Optional override for server budgeting; otherwise derives from LLAMA_N_CTX/VLLM_MAX_MODEL_LEN
# FRONTEND_CTX_LIMIT=8192

# --- Sampling defaults (lowered for coherence) ---
# Temperature and top_p are applied by the frontend Node server for both
# /v1/chat/completions and Harmony /v1/completions paths.
FRONTEND_TEMP=0.0
FRONTEND_TOP_P=0.4
# Repetition controls (OpenAI-compatible backends)
FRONTEND_PRESENCE_PENALTY=0.0
FRONTEND_FREQUENCY_PENALTY=0.2
# Auto-continue attempts for streamed responses (0 disables continuation)
# Auto-continue attempts for streamed responses (0 disables continuation)
FRONTEND_CONT_ATTEMPTS=2

# --- Optional inference gateway (disabled by default) ---
FGK_USE_INFERENCE=0
FGK_INFER_URL=http://localhost:8080
FGK_INFER_KEY=dev-key

# --- Built-in tool runners (server-side) ---
# Enable shell tools for automation in dev (use with care):
FRONTEND_ENABLE_POWERSHELL=1
FRONTEND_ENABLE_BASH=1
# Default working directories for shell tools (inside container)
# PWSH_CWD=/workspace/sandbox
# BASH_CWD=/workspace/sandbox

# --- T11: Tool Execution Sandbox and Gating ---
# Global toggle for tool execution (1=enabled, 0=disabled)
# When disabled, all tool calls return a gated error with telemetry
TOOLS_EXECUTION_ENABLED=1
# Tool execution timeout in milliseconds (default: 30000 = 30 seconds)
TOOL_TIMEOUT_MS=30000
# Maximum retries for failed tool executions (default: 0 = no retries)
TOOL_MAX_RETRIES=0
# Maximum output size in bytes (default: 1048576 = 1MB)
# Outputs exceeding this will be truncated with a warning
TOOL_MAX_OUTPUT_BYTES=1048576

# --- Tool sandbox root (frontend server) ---
# When running the UI container, tool file operations (read_dir/read_file/write_file)
# operate under this sandbox root. Set to a path under a bind-mounted directory so
# outputs persist outside the container. The docker-compose defaults this to
# /workspace/sandbox; override here if needed.
# TOOLS_FS_ROOT=/workspace/sandbox

# --- Repository Write Access (write_repo_file tool) ---
# Enable repository file writing (⚠️ USE ONLY IN LOCAL DEV SANDBOX)
FRONTEND_ENABLE_REPO_WRITE=1
# Comma-separated allowlist of file patterns the agent can modify
# Default is very restrictive (only Dockerfile and docker-compose.yml)
# For local development, you can expand this to enable autonomous code updates:
# REPO_WRITE_ALLOW=frontend/Dockerfile,docker-compose.yml,frontend/src/**/*.tsx,frontend/src/**/*.ts,frontend/src/**/*.jsx,frontend/src/**/*.js,frontend/src/**/*.mjs,frontend/src/**/*.css,frontend/tests/**/*.mjs,frontend/tests/**/*.js,frontend/tools/**/*.mjs,frontend/core/**/*.mjs,frontend/package.json,docs/**/*.md,README.md,.env.example,forgekeeper/**/*.py,scripts/**/*.sh,scripts/**/*.ps1,.github/**/*.yml,tasks.md

# --- T22: Per-Request Rate Limiting (Token Bucket) ---
# Enable rate limiting for chat endpoints (default: 1 = enabled)
RATE_LIMIT_ENABLED=1
# Maximum tokens (burst size) - allows this many requests before throttling
RATE_LIMIT_CAPACITY=100
# Tokens refilled per second (steady-state throughput)
RATE_LIMIT_REFILL_RATE=10
# Tokens consumed per request (default: 1)
RATE_LIMIT_COST_PER_REQUEST=1

# --- Database (future backend) ---
MONGO_URI=mongodb://localhost:27017/forgekeeper?directConnection=true&retryWrites=false

# --- OpenAI key (only when calling external OpenAI) ---
# OPENAI_API_KEY=sk-...

# --- Self-Review Iteration (M2: Quality Improvement) ---
# Enable iterative self-review to improve response quality
FRONTEND_ENABLE_REVIEW=0
# Number of review passes (1-5, default 3)
FRONTEND_REVIEW_ITERATIONS=3
# Quality threshold to accept response (0.0-1.0, default 0.7)
FRONTEND_REVIEW_THRESHOLD=0.7
# Maximum regeneration attempts per review cycle (default 2)
FRONTEND_REVIEW_MAX_REGENERATIONS=2
# Token budget for review evaluation (default 512)
FRONTEND_REVIEW_EVAL_TOKENS=512
# Review mode controls when review is triggered:
#   - always: Review all responses (may slow down tool execution)
#   - never: Disable review entirely
#   - on_error: Only review when errors occur
#   - on_incomplete: Only review incomplete responses
#   - on_complex: Only review text-only responses, skip when tools are used (RECOMMENDED)
# Default: on_complex (prevents review loop deadlock for tool-heavy requests)
FRONTEND_REVIEW_MODE=on_complex

# --- Chunked Reasoning (M2: Overcome Context Limits) ---
# Enable chunked response generation for comprehensive outputs
FRONTEND_ENABLE_CHUNKED=0
# Maximum chunks per response (2-10, default 5)
FRONTEND_CHUNKED_MAX_CHUNKS=5
# Target tokens per chunk (default 1024)
FRONTEND_CHUNKED_TOKENS_PER_CHUNK=1024
# Minimum tokens to trigger auto-chunking (default 2048)
FRONTEND_CHUNKED_AUTO_THRESHOLD=2048
# Auto-outline: let model determine chunk count (1) or use fixed count (0)
FRONTEND_CHUNKED_AUTO_OUTLINE=1
# Maximum outline generation attempts (default 2)
FRONTEND_CHUNKED_OUTLINE_RETRIES=2
# Token budget for outline generation (default 512)
FRONTEND_CHUNKED_OUTLINE_TOKENS=512
# Review each chunk individually (0) or only final assembly (default 0)
FRONTEND_CHUNKED_REVIEW_PER_CHUNK=0

# --- Combined Mode (Review + Chunked) ---
# Strategy when both features enabled: per_chunk, final_only, both (default: final_only)
FRONTEND_COMBINED_REVIEW_STRATEGY=final_only
# Auto-detection: enable review mode automatically based on heuristics (default 0)
FRONTEND_AUTO_REVIEW=0
# Auto-detection: enable chunked mode automatically based on heuristics (default 0)
FRONTEND_AUTO_CHUNKED=0

# --- Phase 4: Autonomous Agent Mode ---
# Enable autonomous agent mode (default: 0)
FRONTEND_ENABLE_AUTONOMOUS_MODE=1
# Maximum iterations for autonomous mode (default: 15)
AUTONOMOUS_MAX_ITERATIONS=15
# Checkpoint interval - iterations between progress updates (default: 5)
AUTONOMOUS_CHECKPOINT_INTERVAL=5
# Error threshold before stopping (default: 3)
AUTONOMOUS_ERROR_THRESHOLD=3
# Playground/sandbox directory for autonomous work (default: .forgekeeper/playground)
AUTONOMOUS_PLAYGROUND_ROOT=.forgekeeper/playground

# --- Phase 7: Multi-Step Lookahead and Proactive Planning ---
# Enable alternative generation for planning (default: 1)
AUTONOMOUS_ENABLE_ALTERNATIVES=1
# Enable lookahead task graph building (default: 1)
AUTONOMOUS_ENABLE_LOOKAHEAD=1
# Lookahead depth - how many steps to plan ahead (default: 3)
AUTONOMOUS_LOOKAHEAD_DEPTH=3
# Minimum alternatives to generate per decision (default: 3)
AUTONOMOUS_MIN_ALTERNATIVES=3
# Effort weight in decision scoring (default: 0.4)
AUTONOMOUS_EFFORT_WEIGHT=0.4
# Risk weight in decision scoring (default: 0.3)
AUTONOMOUS_RISK_WEIGHT=0.3
# Alignment weight in decision scoring (default: 0.3)
AUTONOMOUS_ALIGNMENT_WEIGHT=0.3

# --- TGT: Telemetry-Driven Task Generation ---
# Enable TGT task generation (default: 1)
TASKGEN_ENABLED=1
# Analysis window in minutes (default: 60)
TASKGEN_WINDOW_MIN=60
# Minimum confidence to suggest task (0.0-1.0, default: 0.7)
TASKGEN_MIN_CONFIDENCE=0.7
# Minimum continuations before triggering (default: 5)
TASKGEN_CONT_MIN=5
# Continuation ratio threshold (0.0-1.0, default: 0.15 = 15%)
TASKGEN_CONT_RATIO_THRESHOLD=0.15
# Enable auto-approval of high-confidence tasks (default: 0)
TASKGEN_AUTO_APPROVE=0
# Confidence threshold for auto-approval (0.0-1.0, default: 0.9)
TASKGEN_AUTO_APPROVE_CONFIDENCE=0.9
# Maximum tasks to generate per analysis run (default: 10)
TASKGEN_MAX_TASKS=10

# --- SAPL: Safe Auto-PR Loop ---
# Enable SAPL auto-PR creation (default: 0, kill-switch)
AUTO_PR_ENABLED=0
# Dry-run mode - preview only, no actual git operations (default: 1)
AUTO_PR_DRYRUN=1
# Comma-separated allowlist of file patterns for PR creation
# Default is restrictive (docs, tests, examples only)
AUTO_PR_ALLOW=README.md,docs/**/*.md,*.env.example,tests/**/*.mjs,examples/**/*
# Enable auto-merge for created PRs (default: 0)
AUTO_PR_AUTOMERGE=0

# --- MIP: Metrics-Informed Prompting ---
# Enable metrics-informed prompting hints (default: 1)
PROMPTING_HINTS_ENABLED=1
# Analysis window in minutes (default: 10)
PROMPTING_HINTS_MINUTES=10
# Continuation rate threshold to trigger hints (0.0-1.0, default: 0.15 = 15%)
PROMPTING_HINTS_THRESHOLD=0.15
# Minimum samples needed for analysis (default: 5)
PROMPTING_HINTS_MIN_SAMPLES=5

# --- Scout Metrics ---
# Enable Scout performance metrics collection (default: 1)
SCOUT_ENABLED=1
# Retention period for metrics in hours (default: 24)
SCOUT_RETENTION_HOURS=24

# --- Thought World ---
# Enable Thought World simulation mode (default: 1)
THOUGHT_WORLD_ENABLED=1
# Maximum depth for counterfactual reasoning (default: 5)
THOUGHT_WORLD_MAX_DEPTH=5

# --- ContextLog ---
# Directory for ContextLog JSONL files (default: .forgekeeper/context_log)
FGK_CONTEXTLOG_DIR=.forgekeeper/context_log
# Maximum bytes per ContextLog file before rotation (default: 10485760 = 10MB)
FGK_CONTEXTLOG_MAX_BYTES=10485760
# Retention period in days (default: 7)
FGK_CONTEXTLOG_RETENTION_DAYS=7

# --- Tool System ---
# Sandbox root for file system tools (default: .forgekeeper/sandbox)
TOOLS_FS_ROOT=.forgekeeper/sandbox
# Comma-separated allowlist of tool names (default: all tools enabled)
# Example: TOOL_ALLOW=get_time,echo,read_dir,read_file,write_file
# TOOL_ALLOW=
