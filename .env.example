# Example environment configuration for Forgekeeper

# Preferred FK_* variables
FK_LLM_IMPL=transformers            # or vllm
FK_MODEL_PATH=/path/to/model        # model path or repository
FK_DTYPE=bf16                       # default precision
FK_DEVICE=cuda:0                    # e.g. cpu or cuda:0
#FK_API_BASE=http://localhost:8000/v1  # Base URL for vLLM endpoints

# Legacy LLM_* variables (still supported)
LLM_MODEL_PATH=/path/to/wizardcoder-python-34b.gguf
LLM_CORE_PATH=/path/to/mistral-nemo-instruct.gguf
LLM_CODER_PATH=/path/to/codellama-python.gguf
LLM_THREADS=8
LLM_GPU_LAYERS=-1
LLM_CONTEXT_SIZE=8192
LLM_MAX_TOKENS=2048
LLM_TEMPERATURE=0.7
DEFAULT_PROMPT_MODE=inst
DEFAULT_SYS_PROMPT="You are ForgeKeeper, a helpful AI assistant and code crafter."
ENABLE_FUZZY_COMMANDS=True
ENABLE_LLM_COMMAND_PARSING=False
LLM_BACKEND=llamacpp
# Uncomment and set your OpenAI key if using OpenAI backend
#OPENAI_API_KEY=sk-...
# Path to a local Harmony conversation model (e.g. gpt-oss)
OPENAI_MODEL_PATH=/path/to/gpt-oss.gguf
OPENAI_MODEL=gpt-4o
USE_REFLECTIVE_CORE=true
