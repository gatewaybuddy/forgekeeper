# Example environment configuration for Docker-based development

# Core selection (default: llama). Options: llama, vllm
FK_CORE_KIND=llama

# --- Ports ---
FRONTEND_PORT=5173
BACKEND_PORT=8000
PYTHON_PORT=5000

# --- Frontend runtime (dev) ---
FRONTEND_BACKEND_URL=http://backend:8000
VITE_BACKEND_URL=http://localhost:8000

# --- llama.cpp Core (OpenAI-compatible default) ---
# Uses the official llama.cpp server image with OpenAI API enabled via --api
LLAMA_DOCKER_IMAGE=ghcr.io/ggerganov/llama.cpp:server
LLAMA_PORT_CORE=8001
LLAMA_CONTAINER_PORT=8000
LLAMA_MODEL_CORE=/models/your-model.gguf
LLAMA_N_CTX=8192
# Optional rope scaling to enable >8k contexts (use with care)
LLAMA_ROPE_SCALING_TYPE=none      # options: none, linear, yarn
LLAMA_ROPE_FREQ_BASE=10000.0
LLAMA_ROPE_FREQ_SCALE=1.0
LLAMA_N_GPU_LAYERS=0

# Host directories mounted into the llama.cpp container
LLAMA_MODELS_HOST_DIR=./models

# Optional: Hugging Face token for gated models
# HUGGING_FACE_HUB_TOKEN=hf_...

# Service discovery
FK_CORE_API_BASE=http://llama-core:8000

# --- Frontend Node server runtime ---
# Upstream OpenAI-compatible base the UI server proxies to.
FRONTEND_VLLM_API_BASE=http://llama-core:8000/v1
FRONTEND_VLLM_MODEL=core

# Tooling guardrails (server.mjs)
# Comma-separated list of allowed tool names (default: all in registry)
# TOOL_ALLOW=get_time,echo,read_dir,read_file
# Rate limit for /api/chat (requests per minute per IP). 0 disables limiting.
API_RATE_PER_MIN=60

# Harmony rendering mode
# When set to 1 (default), the server uses a minimal Harmony prompt that only
# pre-fills the final channel. Set to 0 to include the full policy section that
# instructs the model to write private reasoning in the analysis channel; the
# server extracts that as `assistant.reasoning` for the UI.
FRONTEND_USE_HARMONY=1
FRONTEND_HARMONY_MINIMAL=0
FRONTEND_DISABLE_SYSTEM=0

# --- Optional: vLLM Core (kept for compatibility; disabled by default) ---
VLLM_DOCKER_IMAGE=vllm/vllm-openai:latest
VLLM_PORT_CORE=8001
VLLM_CONTAINER_PORT=8000
VLLM_MODEL_CORE=/models/gpt-oss-20b
VLLM_TP=1
VLLM_DTYPE=bfloat16
VLLM_MAX_MODEL_LEN=32768
VLLM_GPU_MEMORY_UTILIZATION=0.9
VLLM_MAX_NUM_BATCHED_TOKENS=4096
VLLM_HF_CACHE_HOST_DIR=./volumes/vllm-cache
VLLM_MODELS_HOST_DIR=./models
VLLM_LOGS_HOST_DIR=./volumes/vllm-logs

# --- Frontend generation budgets ---
# Aim high by default; upstream/model context may cap further.
FRONTEND_MAX_TOKENS=8192
FRONTEND_CONT_TOKENS=4096
# Optional override for server budgeting; otherwise derives from LLAMA_N_CTX/VLLM_MAX_MODEL_LEN
# FRONTEND_CTX_LIMIT=8192

# --- Sampling defaults (lowered for coherence) ---
# Temperature and top_p are applied by the frontend Node server for both
# /v1/chat/completions and Harmony /v1/completions paths.
FRONTEND_TEMP=0.0
FRONTEND_TOP_P=0.4
# Repetition controls (OpenAI-compatible backends)
FRONTEND_PRESENCE_PENALTY=0.0
FRONTEND_FREQUENCY_PENALTY=0.2
# Auto-continue attempts for streamed responses (0 disables continuation)
# Auto-continue attempts for streamed responses (0 disables continuation)
FRONTEND_CONT_ATTEMPTS=2

# --- Optional inference gateway (disabled by default) ---
FGK_USE_INFERENCE=0
FGK_INFER_URL=http://localhost:8080
FGK_INFER_KEY=dev-key

# --- Built-in tool runners (server-side) ---
# Enable shell tools for automation in dev (use with care):
FRONTEND_ENABLE_POWERSHELL=1
FRONTEND_ENABLE_BASH=1
# Default working directories for shell tools (inside container)
# PWSH_CWD=/workspace/sandbox
# BASH_CWD=/workspace/sandbox

# --- Tool sandbox root (frontend server) ---
# When running the UI container, tool file operations (read_dir/read_file/write_file)
# operate under this sandbox root. Set to a path under a bind-mounted directory so
# outputs persist outside the container. The docker-compose defaults this to
# /workspace/sandbox; override here if needed.
# TOOLS_FS_ROOT=/workspace/sandbox

# --- Repository Write Access (write_repo_file tool) ---
# Enable repository file writing (⚠️ USE ONLY IN LOCAL DEV SANDBOX)
FRONTEND_ENABLE_REPO_WRITE=1
# Comma-separated allowlist of file patterns the agent can modify
# Default is very restrictive (only Dockerfile and docker-compose.yml)
# For local development, you can expand this to enable autonomous code updates:
# REPO_WRITE_ALLOW=frontend/Dockerfile,docker-compose.yml,frontend/src/**/*.tsx,frontend/src/**/*.ts,frontend/src/**/*.jsx,frontend/src/**/*.js,frontend/src/**/*.mjs,frontend/src/**/*.css,frontend/tests/**/*.mjs,frontend/tests/**/*.js,frontend/tools/**/*.mjs,frontend/core/**/*.mjs,frontend/package.json,docs/**/*.md,README.md,.env.example,forgekeeper/**/*.py,scripts/**/*.sh,scripts/**/*.ps1,.github/**/*.yml,tasks.md

# --- Database (future backend) ---
MONGO_URI=mongodb://localhost:27017/forgekeeper?directConnection=true&retryWrites=false

# --- OpenAI key (only when calling external OpenAI) ---
# OPENAI_API_KEY=sk-...

# --- Self-Review Iteration (M2: Quality Improvement) ---
# Enable iterative self-review to improve response quality
FRONTEND_ENABLE_REVIEW=0
# Number of review passes (1-5, default 3)
FRONTEND_REVIEW_ITERATIONS=3
# Quality threshold to accept response (0.0-1.0, default 0.7)
FRONTEND_REVIEW_THRESHOLD=0.7
# Maximum regeneration attempts per review cycle (default 2)
FRONTEND_REVIEW_MAX_REGENERATIONS=2
# Token budget for review evaluation (default 512)
FRONTEND_REVIEW_EVAL_TOKENS=512
# Review mode controls when review is triggered:
#   - always: Review all responses (may slow down tool execution)
#   - never: Disable review entirely
#   - on_error: Only review when errors occur
#   - on_incomplete: Only review incomplete responses
#   - on_complex: Only review text-only responses, skip when tools are used (RECOMMENDED)
# Default: on_complex (prevents review loop deadlock for tool-heavy requests)
FRONTEND_REVIEW_MODE=on_complex

# --- Chunked Reasoning (M2: Overcome Context Limits) ---
# Enable chunked response generation for comprehensive outputs
FRONTEND_ENABLE_CHUNKED=0
# Maximum chunks per response (2-10, default 5)
FRONTEND_CHUNKED_MAX_CHUNKS=5
# Target tokens per chunk (default 1024)
FRONTEND_CHUNKED_TOKENS_PER_CHUNK=1024
# Minimum tokens to trigger auto-chunking (default 2048)
FRONTEND_CHUNKED_AUTO_THRESHOLD=2048
# Auto-outline: let model determine chunk count (1) or use fixed count (0)
FRONTEND_CHUNKED_AUTO_OUTLINE=1
# Maximum outline generation attempts (default 2)
FRONTEND_CHUNKED_OUTLINE_RETRIES=2
# Token budget for outline generation (default 512)
FRONTEND_CHUNKED_OUTLINE_TOKENS=512
# Review each chunk individually (0) or only final assembly (default 0)
FRONTEND_CHUNKED_REVIEW_PER_CHUNK=0

# --- Combined Mode (Review + Chunked) ---
# Strategy when both features enabled: per_chunk, final_only, both (default: final_only)
FRONTEND_COMBINED_REVIEW_STRATEGY=final_only
# Auto-detection: enable review mode automatically based on heuristics (default 0)
FRONTEND_AUTO_REVIEW=0
# Auto-detection: enable chunked mode automatically based on heuristics (default 0)
FRONTEND_AUTO_CHUNKED=0

# --- Phase 4: Autonomous Agent Mode ---
# Enable autonomous agent mode (default: 0)
FRONTEND_ENABLE_AUTONOMOUS_MODE=1
# Maximum iterations for autonomous mode (default: 15)
AUTONOMOUS_MAX_ITERATIONS=15
# Checkpoint interval - iterations between progress updates (default: 5)
AUTONOMOUS_CHECKPOINT_INTERVAL=5
# Error threshold before stopping (default: 3)
AUTONOMOUS_ERROR_THRESHOLD=3
# Playground/sandbox directory for autonomous work (default: .forgekeeper/playground)
AUTONOMOUS_PLAYGROUND_ROOT=.forgekeeper/playground
