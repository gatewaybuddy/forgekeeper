version: "3.9"
services:
  mongodb:
    image: mongo:6
    profiles: [backend]
    networks: [forgekeeper-net]
    ports: ["27017:27017"]
    volumes:
      - mongo-data:/data/db

  vllm-core:
    image: forgekeeper-vllm
    profiles: [inference]
    build:
      context: .
      dockerfile: Dockerfile.vllm
      args:
        VLLM_BASE_IMAGE: ${VLLM_DOCKER_IMAGE:-vllm/vllm-openai:latest}
    env_file: ./.env
    environment:
      HF_HOME: /root/.cache/huggingface
      HUGGINGFACE_HUB_CACHE: /root/.cache/huggingface
      VLLM_LOG_DIR: /var/log/vllm
    networks: [forgekeeper-net]
    ports: ["${VLLM_PORT_CORE:-8001}:${VLLM_CONTAINER_PORT:-8000}"]
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - ${VLLM_HF_CACHE_HOST_DIR:-./volumes/vllm-cache}:/root/.cache/huggingface
      - ${VLLM_MODELS_HOST_DIR:-./volumes/vllm-models}:/models
      - ${VLLM_LOGS_HOST_DIR:-./volumes/vllm-logs}:/var/log/vllm
    entrypoint: ["python3","-m","vllm.entrypoints.openai.api_server"]
    command: [
      "--host","0.0.0.0",
      "--port","${VLLM_CONTAINER_PORT:-8000}",
      "--model","${VLLM_MODEL_CORE}",
      "--tensor-parallel-size","${VLLM_TP:-1}",
      "--max-model-len","${VLLM_MAX_MODEL_LEN:-4096}",
      "--gpu-memory-utilization","${VLLM_GPU_MEMORY_UTILIZATION:-0.9}"
    ]

  vllm-coder:
    image: forgekeeper-vllm
    profiles: [inference]
    build:
      context: .
      dockerfile: Dockerfile.vllm
      args:
        VLLM_BASE_IMAGE: ${VLLM_DOCKER_IMAGE:-vllm/vllm-openai:latest}
    env_file: ./.env
    environment:
      HF_HOME: /root/.cache/huggingface
      HUGGINGFACE_HUB_CACHE: /root/.cache/huggingface
      VLLM_LOG_DIR: /var/log/vllm
    networks: [forgekeeper-net]
    ports: ["${VLLM_PORT_CODER:-8002}:${VLLM_CONTAINER_PORT:-8000}"]
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - ${VLLM_HF_CACHE_HOST_DIR:-./volumes/vllm-cache}:/root/.cache/huggingface
      - ${VLLM_MODELS_HOST_DIR:-./volumes/vllm-models}:/models
      - ${VLLM_LOGS_HOST_DIR:-./volumes/vllm-logs}:/var/log/vllm
    entrypoint: ["python3","-m","vllm.entrypoints.openai.api_server"]
    command: [
      "--host","0.0.0.0",
      "--port","${VLLM_CONTAINER_PORT:-8000}",
      "--model","${VLLM_MODEL_CODER}",
      "--tensor-parallel-size","${VLLM_TP:-1}",
      "--max-model-len","${VLLM_MAX_MODEL_LEN:-4096}",
      "--gpu-memory-utilization","${VLLM_GPU_MEMORY_UTILIZATION:-0.9}"
    ]

  backend:
    image: forgekeeper-backend
    build:
      context: ./backend
    profiles: [backend]
    env_file: ./.env
    environment:
      DATABASE_URL: mongodb://mongodb:27017/forgekeeper
      PORT: ${BACKEND_PORT}
    networks: [forgekeeper-net]
    ports: ["${BACKEND_PORT}:${BACKEND_PORT}"]
    depends_on:
      - mongodb

  backend-worker:
    image: forgekeeper-backend
    profiles: [agent-worker]
    env_file: ./.env
    environment:
      DATABASE_URL: mongodb://mongodb:27017/forgekeeper
      MQTT_BROKER: mqtt
    command: ["npm","run","worker","--prefix","backend"]
    networks: [forgekeeper-net]
    depends_on:
      - backend

  frontend:
    image: forgekeeper-frontend
    profiles: [ui]
    env_file: ./.env
    build:
      context: ./frontend
      args:
        BACKEND_URL: ${FRONTEND_BACKEND_URL}
    environment:
      BACKEND_URL: ${FRONTEND_BACKEND_URL}
    networks: [forgekeeper-net]
    ports: ["${FRONTEND_PORT}:80"]

  python:
    image: forgekeeper-python
    build:
      context: .
      dockerfile: Dockerfile
    profiles: [agent]
    env_file: ./.env
    networks: [forgekeeper-net]
    ports: ["${PYTHON_PORT}:${PYTHON_PORT}"]
    depends_on:
      - vllm-core
      - vllm-coder

networks:
  forgekeeper-net:
    external: true

volumes:
  mongo-data:
