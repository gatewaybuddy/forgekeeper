services:
  vllm-core:
    image: forgekeeper-vllm
    profiles: [inference]
    build:
      context: ./archive
      dockerfile: Dockerfile.vllm
    env_file: ./.env
    environment:
      HF_HOME: /root/.cache/huggingface
      HUGGINGFACE_HUB_CACHE: /root/.cache/huggingface
      VLLM_LOG_DIR: /var/log/vllm
      # baseline env only; no forced CUDA arch or attention overrides
    ports:
      - "${VLLM_PORT_CORE:-8001}:${VLLM_CONTAINER_PORT:-8000}"
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - ${VLLM_HF_CACHE_HOST_DIR:-./volumes/vllm-cache}:/root/.cache/huggingface
      - ${VLLM_MODELS_HOST_DIR:-./models}:/models
      - ${VLLM_LOGS_HOST_DIR:-./volumes/vllm-logs}:/var/log/vllm
    networks: [forgekeeper-net]
    entrypoint: ["python3","-m","vllm.entrypoints.openai.api_server"]
    command: [
      "--host","0.0.0.0",
      "--port","${VLLM_CONTAINER_PORT:-8000}",
      "--model","${VLLM_MODEL_CORE}",
      "--served-model-name","core",
      "--tensor-parallel-size","${VLLM_TP:-1}",
      "--max-model-len","${VLLM_MAX_MODEL_LEN:-4096}",
      "--gpu-memory-utilization","${VLLM_GPU_MEMORY_UTILIZATION:-0.9}"
    ]

  frontend:
    image: forgekeeper-frontend
    profiles: [ui]
    build:
      context: ./frontend
    environment:
      FRONTEND_VLLM_API_BASE: ${FRONTEND_VLLM_API_BASE:-http://vllm-core:8000/v1}
      FRONTEND_VLLM_MODEL: ${FRONTEND_VLLM_MODEL:-core}
      PORT: 3000
    ports:
      - "${FRONTEND_PORT:-3000}:3000"
    networks: [forgekeeper-net]

networks:
  forgekeeper-net:
    external: true
