services:
  # Default inference core (llama.cpp C++ server, GPU/cuBLAS)
  llama-core:
    image: ${LLAMA_DOCKER_GPU_IMAGE:-ghcr.io/ggml-org/llama.cpp:server-cuda}
    profiles: [inference]
    env_file: ./.env
    ports:
      - "${LLAMA_PORT_CORE:-8001}:${LLAMA_CONTAINER_PORT:-8080}"
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - ${LLAMA_MODELS_HOST_DIR:-./models}:/models
    networks: [forgekeeper-net]
    command: [
      "-m","${LLAMA_MODEL_CORE}",
      "--host","0.0.0.0",
      "--port","${LLAMA_CONTAINER_PORT:-8080}",
      "-c","${LLAMA_N_CTX:-4096}",
      "--n-gpu-layers","${LLAMA_N_GPU_LAYERS:--1}",
      "--jinja"
    ]

  # CPU fallback (LocalAI) when GPU not available
  llama-core-cpu:
    image: ${LOCALAI_DOCKER_IMAGE:-localai/localai:latest}
    profiles: [inference-cpu]
    env_file: ./.env
    environment:
      MODELS_PATH: /models
    ports:
      - "${LLAMA_PORT_CORE:-8001}:${LLAMA_CONTAINER_PORT:-8080}"
    ipc: host
    volumes:
      - ${LLAMA_MODELS_HOST_DIR:-./models}:/models
    networks: [forgekeeper-net]

  # Optional vLLM Core (kept for compatibility; not started by default)
  vllm-core:
    image: ${VLLM_DOCKER_IMAGE:-vllm/vllm-openai:latest}
    profiles: [inference-vllm]
    env_file: ./.env
    environment:
      HF_HOME: /root/.cache/huggingface
      HUGGINGFACE_HUB_CACHE: /root/.cache/huggingface
      VLLM_LOG_DIR: /var/log/vllm
    ports:
      - "${VLLM_PORT_CORE:-8001}:${VLLM_CONTAINER_PORT:-8000}"
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - ${VLLM_HF_CACHE_HOST_DIR:-./volumes/vllm-cache}:/root/.cache/huggingface
      - ${VLLM_MODELS_HOST_DIR:-./models}:/models
      - ${VLLM_LOGS_HOST_DIR:-./volumes/vllm-logs}:/var/log/vllm
    networks: [forgekeeper-net]
    entrypoint: ["python3","-m","vllm.entrypoints.openai.api_server"]
    command: [
      "--host","0.0.0.0",
      "--port","${VLLM_CONTAINER_PORT:-8000}",
      "--dtype","${VLLM_DTYPE:-float16}",
      "--model","${VLLM_MODEL_CORE}",
      "--served-model-name","core",
      "--tensor-parallel-size","${VLLM_TP:-1}",
      "--max-model-len","${VLLM_MAX_MODEL_LEN:-32768}",
      "--gpu-memory-utilization","${VLLM_GPU_MEMORY_UTILIZATION:-0.9}",
      "--max-num-batched-tokens","${VLLM_MAX_NUM_BATCHED_TOKENS:-4096}"
    ]

  frontend:
    image: forgekeeper-frontend
    pull_policy: build
    profiles: [ui]
    build:
      context: ./frontend
      args:
        VITE_VLLM_API_BASE: ${FRONTEND_VLLM_API_BASE:-http://llama-core:8000/v1}
        VITE_VLLM_MODEL: ${FRONTEND_VLLM_MODEL:-core}
    environment:
      FRONTEND_VLLM_API_BASE: ${FRONTEND_VLLM_API_BASE:-http://llama-core:8000/v1}
      FRONTEND_VLLM_MODEL: ${FRONTEND_VLLM_MODEL:-core}
      FRONTEND_ENABLE_POWERSHELL: ${FRONTEND_ENABLE_POWERSHELL:-1}
      # Leave TOOL_ALLOW unset to allow all tools. Set to comma list to restrict.
      # TOOL_ALLOW: "get_time,echo,read_dir"
      PORT: 3000
    ports:
      - "${FRONTEND_PORT:-3000}:3000"
    networks: [forgekeeper-net]

networks:
  forgekeeper-net:
    external: true
