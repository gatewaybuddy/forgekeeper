# TGT Week 2 Implementation Summary

**Date**: 2025-11-03
**Status**: âœ… Complete
**Phase**: Heuristic Analyzers

---

## Overview

Completed Week 2 deliverables for TGT (Telemetry-Driven Task Generator), implementing all 5 heuristic analyzers and integrating them with the demo script for end-to-end testing.

---

## Deliverables

### 1. Error Spike Analyzer âœ…
**File**: `frontend/core/taskgen/analyzers/error-spike.mjs` (182 lines)

**Purpose**: Detect sudden increases in tool or upstream errors compared to baseline

**Trigger Conditions**:
- Error count exceeds baseline by 3x or more (configurable)
- At least 5 errors in time window

**Severity Mapping**:
- Critical: â‰¥5x baseline
- High: â‰¥4.5x baseline
- Medium: â‰¥3x baseline

**Key Features**:
- Compares current error rate (errors/hour) to 7-day baseline
- Groups errors by type/name to identify top error
- Provides sample error events for evidence
- Determines likely affected files based on error patterns
- Suggests investigation and mitigation steps

**Evidence Collected**:
- Current error count and rate
- Historical baseline (7-day average)
- Error rate multiplier
- Top error type and frequency
- Sample error events (top 3)
- Total error types detected

**Example Output**:
```
ðŸ”´ CRITICAL
Investigate 10.0x error spike: 'read_file' failures

Evidence:
  current: 50 errors
  baseline: 5.0/hour
  ratio: 10.0x
  threshold: 3x
  topError: read_file (30 occurrences)

Suggested Fix:
  Approach: investigate_and_mitigate
  Files: frontend/tools/filesystem.mjs, frontend/server.tools.mjs
  Changes:
    - Check recent changes to affected components
    - Add error handling or validation
    - Improve error messages with actionable guidance

Priority: 100 | Confidence: 95%
```

---

### 2. Documentation Gap Analyzer âœ…
**File**: `frontend/core/taskgen/analyzers/docs-gap.mjs` (185 lines)

**Purpose**: Detect features, tools, or APIs that lack adequate documentation

**Trigger Conditions**:
- Tool has been called â‰¥20 times (configurable)
- Tool lacks documentation (checked via heuristic)

**Severity Mapping**:
- Critical: â‰¥100 calls
- High: â‰¥50 calls
- Medium: â‰¥20 calls

**Key Features**:
- Groups tool calls by name
- Finds top tools by usage frequency
- Checks for documentation existence (heuristic-based)
- Determines likely documentation files to create/update
- Categorizes tools by type (filesystem, shell, network, git, misc)

**Evidence Collected**:
- Tool name and call count
- Top 5 most-used tools
- Sample tool calls (top 3)
- Total tool calls and unique tools

**Example Output**:
```
ðŸŸ¡ MEDIUM
Add documentation for heavily-used 'undocumented_tool' tool (40 calls)

Evidence:
  tool: undocumented_tool
  callCount: 40
  timeWindow: 60 minutes

Suggested Fix:
  Approach: add_comprehensive_docs
  Files: docs/api/tools.md, docs/tools/undocumented_tool.md
  Changes:
    - Add detailed documentation for 'undocumented_tool' tool
    - Include usage examples and common patterns
    - Document all parameters with types and defaults

Priority: 36 | Confidence: 72%
```

---

### 3. Performance Degradation Analyzer âœ…
**File**: `frontend/core/taskgen/analyzers/performance.mjs` (182 lines)

**Purpose**: Detect performance degradation by comparing current latencies to historical baselines

**Trigger Conditions**:
- p95 latency exceeds baseline by 1.5x or more (configurable)
- At least 20 events with latency data

**Severity Mapping**:
- Critical: â‰¥2.0x baseline
- High: â‰¥1.95x baseline (1.5x * 1.3)
- Medium: â‰¥1.5x baseline

**Key Features**:
- Calculates p95, p50, p99, and average latency
- Compares to 7-day historical baseline
- Identifies bottleneck type (most common slow operation)
- Provides slowest sample events for investigation
- Determines likely affected files based on bottleneck

**Evidence Collected**:
- Current p95 latency
- Historical baseline (7-day average)
- Latency degradation ratio
- Percentile statistics (p50, p95, p99, avg)
- Bottleneck type and count
- Slowest sample events (top 5)

**Example Output**:
```
ðŸ”´ CRITICAL
Investigate 7.5x performance degradation: p95 latency 7544ms

Evidence:
  current: 7544ms
  baseline: 1000ms
  ratio: 7.5x
  avgLatency: 3200ms
  p50: 2100ms
  p99: 9800ms
  bottleneck: assistant_response (8 occurrences)

Suggested Fix:
  Approach: investigate_and_optimize
  Files: frontend/server.orchestrator.mjs
  Changes:
    - Profile assistant_response operations to identify bottlenecks
    - Check for database query inefficiencies
    - Add caching for frequently accessed data

Priority: 100 | Confidence: 95%
```

---

### 4. UX Issue Analyzer âœ…
**File**: `frontend/core/taskgen/analyzers/ux-issue.mjs` (265 lines)

**Purpose**: Detect user experience issues based on conversation patterns

**Trigger Conditions**:
- Conversation abort rate >20% (configurable)
- Long wait rate >15% (>8s wait time)
- Error-heavy conversation rate >25% (>30% errors per conversation)
- At least 10 conversations analyzed

**Severity Mapping**:
- Critical: â‰¥35% abort rate
- High: â‰¥30% abort rate, >30% long wait rate, >40% error-heavy
- Medium: â‰¥20% abort rate, >15% long wait rate, >25% error-heavy

**Key Features**:
- Groups events by conversation ID
- Detects aborted conversations (no successful completion)
- Identifies conversations with long wait times (>8s)
- Finds error-heavy conversations (>30% error rate)
- Creates separate task cards for each UX issue type

**Evidence Collected**:
- Total conversations analyzed
- Abort count and rate
- Long wait count and rate
- Error-heavy count and rate
- Sample slow events

**Example Output**:
```
ðŸŸ  HIGH
Reduce long wait times affecting 45% of users

Evidence:
  current: 45.0%
  threshold: 15%
  waitThreshold: 8s
  timeWindow: 60 minutes
  samples: [5 slow events]

Suggested Fix:
  Approach: optimize_performance
  Files: frontend/server.orchestrator.mjs
  Changes:
    - Add streaming responses for immediate feedback
    - Optimize tool execution paths
    - Add caching for common queries

Priority: 68 | Confidence: 75%
```

---

### 5. Updated Demo Script âœ…
**File**: `frontend/core/taskgen/demo.mjs` (245 lines, executable)

**Changes**:
- Added imports for all 5 analyzers
- Registered all 5 analyzers in registry
- Enhanced synthetic data generation to trigger all analyzers:
  - 200 assistant responses with 20% continuations
  - 50 errors (5x baseline)
  - 100 tool calls (40 undocumented_tool calls)
  - 30 slow events (4-12s latency)
  - 5 aborted conversations
- Added conversation IDs to events
- Added latency data (`elapsed_ms`) to events

**Usage**:
```bash
node frontend/core/taskgen/demo.mjs [--window=60] [--min-confidence=0.7]
```

**Sample Output**:
```
ðŸ”§ TGT Demo - Telemetry-Driven Task Generator

Configuration:
  Time window: 60 minutes
  Min confidence: 60%

ðŸ“ Loading ContextLog events...
âš ï¸  No ContextLog events found. Generating synthetic test data...

ðŸ” Registering analyzers...
[TGT] Registered analyzer: ContinuationAnalyzer
[TGT] Registered analyzer: ErrorSpikeAnalyzer
[TGT] Registered analyzer: DocsGapAnalyzer
[TGT] Registered analyzer: PerformanceAnalyzer
[TGT] Registered analyzer: UXIssueAnalyzer

âš™ï¸  Running analyzers...
[TGT] Running 5 analyzers...

RESULTS: 5 tasks generated

By Severity:
  ðŸ”´ CRITICAL: 2
  ðŸŸ  HIGH: 1
  ðŸŸ¡ MEDIUM: 2

Average Priority: 70.2
Average Confidence: 82.0%
```

---

## File Structure

```
frontend/core/taskgen/
â”œâ”€â”€ taskcard.mjs                     # Task card schema (296 lines) [Week 1]
â”œâ”€â”€ analyzer.mjs                     # Base analyzer & registry (195 lines) [Week 1]
â”œâ”€â”€ contextlog-helpers.mjs           # ContextLog query utilities (313 lines) [Week 1]
â”œâ”€â”€ demo.mjs                         # Demo script (245 lines) [Week 1+2]
â””â”€â”€ analyzers/
    â”œâ”€â”€ continuation.mjs             # Continuation detector (124 lines) [Week 1]
    â”œâ”€â”€ error-spike.mjs              # Error spike detector (182 lines) [Week 2]
    â”œâ”€â”€ docs-gap.mjs                 # Documentation gap detector (185 lines) [Week 2]
    â”œâ”€â”€ performance.mjs              # Performance degradation detector (182 lines) [Week 2]
    â””â”€â”€ ux-issue.mjs                 # UX issue detector (265 lines) [Week 2]
```

**Week 2 Addition**: ~814 lines of code (4 new analyzers)
**Total Lines of Code**: ~1,847 lines (Week 1 + Week 2)

---

## Testing

### End-to-End Demo Test

**Command**:
```bash
node frontend/core/taskgen/demo.mjs --window=60 --min-confidence=0.6
```

**Results**:
- âœ… All 5 analyzers registered successfully
- âœ… Synthetic data generated (385 events total)
- âœ… 5 tasks generated:
  1. Error Spike (CRITICAL, priority 100, 95% confidence)
  2. Performance Degradation (CRITICAL, priority 100, 95% confidence)
  3. UX Issue - Long Waits (HIGH, priority 68, 75% confidence)
  4. Continuation (MEDIUM, priority 47, 73% confidence)
  5. Documentation Gap (MEDIUM, priority 36, 72% confidence)
- âœ… Tasks sorted by priority (100, 100, 68, 47, 36)
- âœ… Summary statistics calculated correctly
- âœ… Average priority: 70.2
- âœ… Average confidence: 82.0%

### Synthetic Data Breakdown

| Category | Count | Purpose |
|----------|-------|---------|
| Assistant responses | 200 | Trigger continuation analyzer (20% continuations) |
| Errors | 50 | Trigger error spike analyzer (5x baseline) |
| Tool calls | 100 | Trigger docs gap analyzer (40 undocumented) |
| Slow events | 30 | Trigger performance analyzer (4-12s latency) |
| Aborted conversations | 5 | Trigger UX analyzer (no completion) |
| **Total events** | **385** | **Comprehensive test coverage** |

---

## Analyzer Comparison

| Analyzer | LOC | Trigger | Severity Logic | Evidence Points | Confidence Formula |
|----------|-----|---------|----------------|-----------------|-------------------|
| **Continuation** | 124 | >15% ratio | >30%=CRIT, >20%=HIGH | 6 | `min(0.95, 0.7 + (ratio - threshold) * 2)` |
| **Error Spike** | 182 | 3x baseline | â‰¥5x=CRIT, â‰¥4.5x=HIGH | 7 | `min(0.95, 0.65 + (multiplier - threshold) * 0.1)` |
| **Docs Gap** | 185 | â‰¥20 calls | â‰¥100=CRIT, â‰¥50=HIGH | 6 | `min(0.95, 0.6 + (count / 100) * 0.3)` |
| **Performance** | 182 | 1.5x baseline | â‰¥2.0x=CRIT, â‰¥1.95x=HIGH | 8 | `min(0.95, 0.6 + (ratio - threshold) * 0.15)` |
| **UX Issue** | 265 | >20% abort | â‰¥35%=CRIT, â‰¥30%=HIGH | 5-7 | `min(0.90, 0.65 + (rate - threshold) * 2)` |

---

## Success Criteria âœ…

| Criterion | Status | Evidence |
|-----------|--------|----------|
| Error spike analyzer implemented | âœ… | 182 lines, full baseline comparison |
| Documentation gap analyzer implemented | âœ… | 185 lines, tool usage tracking |
| Performance analyzer implemented | âœ… | 182 lines, p95 latency monitoring |
| UX issue analyzer implemented | âœ… | 265 lines, multi-pattern detection |
| Demo script updated | âœ… | All 5 analyzers registered, synthetic data |
| End-to-end test passing | âœ… | 5 tasks generated, sorted correctly |

**5 of 5 deliverables complete**

---

## Next Steps (Week 3: API Integration)

### Week 3 Deliverables:
1. **API Endpoints**:
   - `POST /api/tasks/suggest` - Run analyzers and return tasks
   - `GET /api/tasks` - List generated tasks
   - `POST /api/tasks/:id/dismiss` - Dismiss task
   - `POST /api/tasks/:id/approve` - Approve task
   - `GET /api/tasks/:id` - Get task details

2. **Task Persistence**:
   - Store tasks to `.forgekeeper/tasks/generated_tasks.jsonl`
   - Track task status (generated, approved, dismissed, completed)
   - Link tasks to ContextLog events

3. **Scheduled Analysis**:
   - Background job to run analyzers every 15 minutes
   - Configurable via `TASKGEN_ANALYSIS_INTERVAL_MIN` environment variable
   - Rate limiting to avoid spam (max 10 tasks per hour)

4. **Integration Testing**:
   - Test with real ContextLog data
   - Verify task persistence
   - Test API endpoints

---

## Configuration

### Environment Variables (Future)

```bash
# Enable TGT (default: off for safety)
TASKGEN_ENABLED=0  # Set to 1 to enable

# Time window for analysis (minutes)
TASKGEN_WINDOW_MIN=60

# Minimum confidence to generate task (0.0-1.0)
TASKGEN_MIN_CONFIDENCE=0.7

# Max tasks to generate per analysis
TASKGEN_MAX_TASKS=10

# Analysis interval (minutes)
TASKGEN_ANALYSIS_INTERVAL_MIN=15

# Analyzer-specific thresholds
TASKGEN_CONTINUATION_THRESHOLD=0.15        # 15%
TASKGEN_ERROR_SPIKE_MULTIPLIER=3.0         # 3x baseline
TASKGEN_DOCS_GAP_MIN_USAGE=20              # 20 calls
TASKGEN_PERFORMANCE_THRESHOLD=1.5          # 1.5x baseline
TASKGEN_UX_ABORT_THRESHOLD=0.20            # 20% abort rate
```

---

## Performance Considerations

### Memory Usage
- ContextLog loading: O(events in time window)
- Typical: 1,000-10,000 events per hour
- Memory: ~1-10 MB for 60-minute window
- Analyzer processing: ~1-2 MB per analyzer

### Execution Time
- ContextLog loading: 100-500ms (JSONL parsing)
- Each analyzer: 50-200ms
- Total (5 analyzers): <1 second (parallel execution)
- Demo script with synthetic data: ~300-400ms

### Scalability
- Parallel analyzer execution via `Promise.allSettled`
- Incremental JSONL reading (stops when time window exceeded)
- Task generation is stateless (no shared state between runs)
- 5 analyzers can run concurrently on multi-core systems

---

## Known Limitations

1. **No API Integration**: Week 2 focused on analyzers; API endpoints in Week 3
2. **No Task Persistence**: Tasks generated in memory only; JSONL storage in Week 3
3. **Heuristic Documentation Check**: `checkToolDocs()` uses hardcoded list; should scan docs/ directory
4. **Static Baselines**: Baseline calculation works but needs tuning based on production data
5. **No Scheduled Analysis**: Manual execution only; background jobs in Week 3
6. **No UI**: Task cards logged to console; UI drawer in Week 4

---

## Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   TGT Week 2 - Analyzers                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ ContextLog Query Helpers (Week 1)                     â”‚  â”‚
â”‚  â”‚ - Load events from JSONL files                        â”‚  â”‚
â”‚  â”‚ - Filter, group, aggregate                            â”‚  â”‚
â”‚  â”‚ - Calculate baselines and percentiles                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                          â†“                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Analyzer Registry (Week 1)                            â”‚  â”‚
â”‚  â”‚ - Register analyzers                                   â”‚  â”‚
â”‚  â”‚ - Run all enabled analyzers in parallel               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                          â†“                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Heuristic Analyzers (extend BaseAnalyzer)            â”‚  â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚ â”‚ 1. ContinuationAnalyzer (Week 1)                  â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    - Detect incomplete responses                   â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    - Threshold: 15% continuation rate              â”‚ â”‚  â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚ â”‚ 2. ErrorSpikeAnalyzer (Week 2)                    â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    - Detect error rate increases                   â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    - Threshold: 3x baseline                        â”‚ â”‚  â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚ â”‚ 3. DocsGapAnalyzer (Week 2)                       â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    - Detect undocumented tools                     â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    - Threshold: â‰¥20 calls                          â”‚ â”‚  â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚ â”‚ 4. PerformanceAnalyzer (Week 2)                   â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    - Detect latency degradation                    â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    - Threshold: 1.5x baseline (p95)                â”‚ â”‚  â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚ â”‚ 5. UXIssueAnalyzer (Week 2)                       â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    - Detect abort patterns, long waits, errors    â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    - Threshold: 20% abort rate                     â”‚ â”‚  â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                          â†“                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Task Card Generator (Week 1)                          â”‚  â”‚
â”‚  â”‚ - Create validated task cards                         â”‚  â”‚
â”‚  â”‚ - Calculate priority scores                           â”‚  â”‚
â”‚  â”‚ - Sort and filter tasks                               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                          â†“                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Task Cards (JSON)                                      â”‚  â”‚
â”‚  â”‚ [5 tasks generated, sorted by priority]               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## References

- **TGT Design**: `docs/autonomous/tgt_telemetry_driven_task_generator.md`
- **Week 1 Summary**: `docs/autonomous/TGT_WEEK1_IMPLEMENTATION_SUMMARY.md`
- **Self-Improvement Plan**: `docs/roadmap/self_improvement_plan.md`
- **ContextLog ADR**: `docs/contextlog/adr-0001-contextlog.md`

---

**Status**: âœ… Week 2 Complete
**Next**: Week 3 - API Integration & Task Persistence
**Estimated Effort**: Week 3 = 1 week (4 endpoints + persistence + scheduling)
