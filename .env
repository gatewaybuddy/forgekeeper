# .env
LLM_MODEL_PATH=C:/LLM/models/TheBloke/WizardCoder-Python-34B-V1.0-GGUF/wizardcoder-python-34b-v1.0.Q4_K_S.gguf
LLM_CORE_PATH=C:/LLM/models/lmstudio-community/Mistral-Nemo-Instruct-2407-GGUF/Mistral-Nemo-Instruct-2407-Q4_K_M.gguf
LLM_CODER_PATH=C:/LLM/models/TheBloke/CodeLlama-13B-Python-GGUF/codellama-13b-python.Q4_K_S.gguf
LLM_THREADS=8
LLM_GPU_LAYERS=-1
LLM_CONTEXT_SIZE=8192     
# Up to 16384 if you really want
LLM_MAX_TOKENS=2048       
# Or higher if you want longer replies
LLM_TEMPERATURE=0.7
DEFAULT_PROMPT_MODE=inst
DEFAULT_SYS_PROMPT=You are ForgeKeeper, a helpful AI assistant and code crafter.
ENABLE_FUZZY_COMMANDS=True
ENABLE_LLM_COMMAND_PARSING=False
LLM_BACKEND=llamacpp
#openai   
# or: llamacpp
#OPENAI_API_KEY=11111
OPENAI_MODEL=gpt-4o
USE_REFLECTIVE_CORE=true

